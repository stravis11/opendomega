{
  "video_id": "W0-SC0SVW0A",
  "url": "https://www.youtube.com/watch?v=W0-SC0SVW0A",
  "title": "12/4/23 Senate Committees on Public Safety | Science & Technology",
  "chamber": "senate",
  "session_type": "committee",
  "session_year": 2026,
  "day_number": null,
  "video_date": null,
  "duration_minutes": 105,
  "summary": "Here is a structured markdown summary of the Georgia 12/4/23 Senate Committees on Public Safety | Science & Technology session:\n\n**Overview**\nThis session focused on discussing the impacts and policy considerations around artificial intelligence (AI) technology. The discussion emphasized the transformative potential of AI while also highlighting the need for balanced regulation to manage risks and protect against harms.\n\n**Key Actions**\nNo bills were passed or referred during this session.\n\n**Bills Introduced (First Reading)**\nNo new bills were introduced.\n\n**Notable Moments**\n- The session opened with a prayer led by Senator Russ Goodman.\n- The committee chairs, Senator Payne and Representative Thomas, provided opening remarks emphasizing the importance of AI policy and the need for bipartisan collaboration.\n- The Heritage Foundation presented on the need for \"responsible AI policy\" that balances innovation and oversight based on specific use cases and domains.\n\n**Votes**\nNo recorded votes took place.\n\n**Attendance**\nThe committees maintained a quorum throughout the session.\n\n**Adjournment**\nThe session adjourned, with no specific next meeting date provided.",
  "transcript": "you We've got some folks that are going to be rolling in the House and we're going to begin this meeting joint committee meeting between the House of Committee on Artificial Intelligence, the Senate Committee on Science Technology and the Senate Committee on Public Safety. We are excited to kick it off and we're going to start every meeting here as we normally do and I'm going to ask our very own senator and chairman of agriculture Russ Goodman to open us up in order to prayer. Thank you, Mr. Chairman. I pray with me, please. Terri, I want to thank you for the day, Lord. We thank you for loving us. We thank you for the gift of salvation. We thank you for the opportunity to assemble here today, Lord, as we do the people's business. And we just ask that you preside over this meeting, Lord, that you give us wisdom and discernment. And then you give us open minds, Lord, so that we can figure out ways to craft the best policy going forward on behalf of the people of this state that we are so grateful to be able to serve. And Jesus' name we pray. Amen. Amen. Thank you. Okay, as we kick off here, I'm going to make a few comments and then ask if chairman Payne or chairman Thomas to other side here would like to make any comments as well. I am grateful that we have the opportunity convene for our second meeting to review artificial intelligence. This is a critically important issue, not just for our state, before our local, city, county, schools, federal, and candy, it's going to touch everybody around the world. If you've heard this before, please hear it again. We think of artificial intelligence as being a disruptor. Many of us think of a disruptor as when Uber came out versus taxi caps. Or when we got an iPhone with a touchscreen, you replaced those old blackberries. Or even the internet for those of us who remember before it was here. Folks, artificial intelligence is the wheel. It is that transformative to what's going to happen in our daily lives and it's happening literally in real time right now. And we have to be prepared for that. And I think if you look at the work that's being done by both the Senate and the House, we've assembled a lot of people that are beginning the process of first and foremost educating. We need out of the 236 legislators that serve here in the General Assembly, people to be able to come up to speed and understand what artificial intelligence is. It is not brand new. In fact, the concept's been around for over 50 years. What's different is the propelling of technology where it is learning and evolving by itself now. Now with that is going to solve some of the world's greatest problems, which is very exciting. It also poses probably one of the largest threats that we have seen in modern times. So with that, we want to learn, we want to figure out what do we need to do here to help protect the state, our citizens, and the governments that we charter. As well as make sure that we're doing right by not stifling innovation and working together with our partners across all industries and all levels of government. So I was thrilled when originally Senator Payne and I decided we should merge the Science and Technology and Public Safety Committee for the purpose of having these meetings on our official intelligence. And then talking to my good friend and colleague Brad Thomas in the representative where we share part of Cherokee County Reserves as a Cherokee County chairman for our delegation. So proud for that as well as Chairman Todd Jones who just joined us as well. So with that, I'm going to offer a moment for any comments from first chairman Payne anything you'd like to say for all those listening. I thank you pretty much summed it up, Mr. Chairman. All right. How about you, Chairman Thomas? I would second Senator Payne's comment. All right. All in favor. Very good. All right. With that in mind, we're going to jump into our meeting. So quick ground rules. We're going to ask those that come to present today. We've got a couple presenters to come to the podium because we're going to be live streaming as well as recording the message today. When you are done presenting, we will ask you to stay right there. We're going to ask some questions. We promise to go easy on you. And then when it's over, I know that some of you already have, but we'll ask you just whatever information that you can share with us for those that we're not here today as far as presentations. I know some of the stuff you may some you may not. We can get that back to Miss Allison Bailey who is right next to me. That would be greatly appreciated. So with that, let's go ahead and kick it off. And first up, we're going to bring up the heritage foundation. So come on down here and ask everybody to introduce yourselves and anybody you may have brought with you. Yes, please feel free to grab water and we'll kick it off to you. All right. We good to go. Perfect. All right. Well, good afternoon. My name is Jake Denton. And I'm a research associate in the Tech Policy Center at the Heritage Foundation. I wanted to start by thanking you all for affording me the privilege of sharing my perspective on AI policy today and get right into it. So this accompanying PowerPoint for you, calling it crafting, responsible AI policy. So starting with some background. Artificial intelligence has the potential to elevate the state of Georgia by transforming countless industries from reimagining healthcare to optimizing transportation networks. As advanced machine learning systems continue to progress, lawmakers should greet these emerging technologies not with alarm, but optimism and view good policy, not as restricting innovation, but as responsibly guiding it. While prevailing discussions surrounding AI often gravitate towards speculative dangers, the reality is that this technology is overwhelmingly beneficial. The majority of current applications are harmless. And the technology has already made substantial progress towards improving our quality of life. For example, natural language processing algorithms power the ability of systems such as Siri and Alexa to comprehend spoken questions and provide relevant responses. Computer vision AI enables social media platforms to recognize human faces and images and identify the correct individuals to tag. Self-driving vehicles utilize AI to visually perceive road signs, traffic lights, pedestrians, and other objects in the environment. To properly navigate streets on highways and recommended recommendation engines on entertainment platforms such as Netflix and Spotify, employ AI to study individual user behavior and suggest personalized content that aligns with demonstrated preferences and usage history. As lawmakers in Georgia and around the country craft legislation on AI, policies should stay focused on particular applications and use cases of the technology rather than one size fits all solutions. Certain high stakes industries like healthcare warrant tailored governance addressing their risks without blanket restrictions, throttling innovation more broadly. Creating narrowly targeted frameworks balancing oversight and progress enables thriving development across less sensitive domains without forfeiting protections where needed most. The prune path forward is not sweeping constraints on all AI systems, but practical safeguards selectively empowering societal benefits while managing harms by context. With per-sky scoping of domains of concern, policies can elevate both innovation and accountability. So we'll move on to artificial intelligence and transparency in particular pertaining to generative AI. Generative artificial intelligence refers to machine learning systems that can create realistic synthetic images, text, video, or audio content. While these technologies have promising commercial and artistic benefits, they can also be used to manipulate vulnerable consumer groups. As these technologies proliferate across advertising, business, and political domains, crafting policy solutions that ensure consumer protections should be a top priority for legislators. In advertising, generative AI can enable brands and companies to subtly treat subtly tweak product images in ways that can harm consumers. For example, brands leveraging AI image generation techniques to model unrealistic product images that exaggerate or deliberately misrepresent attributes to stimulate purchases. Consider a scenario where a clothing retailer employs this technology to synthetically elevate the visual representation of a shirt, displaying superior quality and style that surpasses the actual product. The absence of explicit disclosures regarding the manipulation or synthetic origin of these images can leave consumers susceptible to being swayed into making purchases under false pretenses. This scenario underscores the critical need for guidelines that require transparency and advertising practices to ensure consumers are awarded the ability to make an informed decision when purchasing a product. Beyond generative AI and advertising, conversational AI systems pose risk to consumers as well. Chatbot-powered by natural language models can subtly impersonate human representatives and mislead customers seeking support. Without transparency that an AI agent is handling service inquiries rather than a human, consumers can easily be deceived and manipulated. As conversational AI becomes more common in customer service, clear disclosures indicating when a chatbot is engaging with a customer will prove critical to avoid eroding public trust. Companies leveraging such technologies for automated support should pair deployment with visible signals confirming when an AI agent not a person is handling requests. With prudent measures embedded at the start, conversational AI can gain trust rather than undermine integrity and enterprise deployments. Lawmakers here in Georgia should consider establishing clear transparency guidelines around the use of AI and in other settings where customers could be deceived, such as customer service. This could include imposing transparency requirements that compel brands to disclose any significant alterations to a product's attributes or the generation of entirely synthetic images. Additionally, this could include requiring clear notifications indicating when a person is communicating with an automated AI powered system rather than a human agent. Similarly, within the political arena, generative AI can enable new forms of voter manipulation and deception. Rival campaigns, foreign entities, and even private citizens can leverage technologies currently available to the public to generate convincing fake videos making candidates seem erratic, unhealthy, or behaving inappropriately. They might also synthesize audio impersonating a candidate's voice to make misleading robo calls to a constituent right before a critical moment in a race such as a state caucus, primary, or general election. Even minor and subtle instances of misinformation enabled through generative AI can damage the reputation of a candidate or public figure when strategically timed and targeted. Given the high risks of synthetic media, given the high risk synthetic media posed for electoral integrity, lawmakers should move swiftly to establish disclosure guidelines around the use of synthetic media and elections. Specifically, lawmakers should consider crafting guidelines for disclaimers and watermarks for any artificial intelligence, image, or video crafted or distributed by candidates or political action committees. Similarly, clear notifications should precede any use of synthetic voice technology and election phone and radio outreach. By compelling unambiguous signaling around AI-enabled communications, the provenance remains transparent regardless of how compelling the simulation is. In these spheres, the core policy issues reside not fundamentally with generative AI itself, but rather with the insufficient safeguards around its use in certain settings. In advertising, business, and political environments where communications deal principally in subtle emotional appeals, your constituents are susceptible to deception and abuse. Legislators should take steps to impose standards for these use cases before Georgians are harmed by the improper use of the technology. If you turn your attention to the slide, you'll see an example of an early case of AI in an electoral context when the RNC released a first of its kind political advertisement targeted by the election campaign. While, as you can see, there are instances of disclosure in the video that were entirely optional. In the slide, in me cases, users did not really even notice it. I did an anecdotal little focus group of, was this AI generated, was it scripted, how did this work, and most people were still loosely at the time under the impression that it was a non-AI generated image, even though that tiny little watermark was in the top of the corner. So, as you craft legislation, it's important to give standards such as centralized disclosures or font-size things of that nature to ensure that guidelines aren't skirted. Turning to data privacy. AI systems can revolutionize workflows and supercharge problem solving for Georgia-based companies across all sectors of the state's economy. By automating complex manual tasks and extracting valuable insights from large data sets, AI can drive major efficiency gains, faster innovations, and cost savings. Supply chain use cases showcase this promise. AI can allow advanced demand forecasting, optimize logistics, and responsive tracking from real-time data. As firms automate tasks previously requiring manual effort or find ways to enhance human roles with AI, the use of AI-powered tools will increase substantially. However, amid increasing integration, AI can also present major risks to businesses. The rise of interactive AI writing assistance and chatbots has brought an increased risk of intellectual property theft and data privacy concerns. Consider a researcher at an engineering firm leveraging an AI assistant to proofread documents detailing proprietary device schematics or manufacturing techniques. Despite the good intent, this single act could enable the theft of invaluable trade secrets built up over the years. In many cases, once the data is consumed by the model, it can be retained, analyzed, and shared without consent or notice to the user. Now envision a thriving Atlanta-based self-driving car startup racing towards a software breakthrough promising immense economic upside for Georgia. A single accidental leak here, like an engineer running the company's code through an AI assistant, could permanently destroy the business. Once exposed to external models, confidential data can rarely be recovered or contained. Such inadvertent breaches pose monumental often incalculable damages to companies. Yet with the convenience and power of these tools, such mistakes can easily occur. Beyond impacts to businesses, private citizens also confront escalating privacy concerns posed by AI-powered systems. Users may unintentionally input sensitive details like social security numbers or medical history into tools expecting security protections. However, without strict data privacy guidelines, minor developer oversights or outright unethical data practices, such as undisclosed data harvesting, could lead to significant cases of data exploitation. As AI adoption accelerates across public and private sectors, lawmakers should look to implement proactive policies for the governance of data collection and retention. Specifically, legislative measures could take steps to clarify what types of data can be collected and retained by AI-powered tools establishing safeguards for sensitive user data. Furthermore, lawmakers should also consider establishing limitations on third-party data sharing without explicit consent of the user. These limitations can help balance innovation with privacy across consumer and business use cases of AI. By establishing such guidelines, lawmakers will also ensure businesses that this technology is safe to use, which could serve as a catalyst for increased private sector adoption. By assuring businesses that the technology adheres to stringent data governance standards, Georgia can foster an environment conducive to innovation, thereby bolstering the competitiveness of its enterprises. If you look to the slides here, you'll see a story from around a year ago that was fairly common. Apple restricted the use of employees from using chat GPT over concerns of data leaks. Particularly, this had to do with code. It's a very powerful tool to pick out little glitches, little bugs in your code that often engineers overlook. So this was a shortcut, and shortcuts are very compelling. There weren't the necessary privacy procedures set in place to give companies like Apple the confidence of using a tool like that. While down the road, Apple may roll out its own AI model, and that may be the reason for not allowing chat GPT in the future in the immediate where companies are looking to innovate, they're having to essentially avoid these tools due to concerns over privacy. And so this is just one element, as you can see, it can also snowball over into personal use as well, but to consider here, we want companies to be able to embrace this technology to make things happen faster. So giving these types of guidelines is a bridge to that reality. Turning to the importance of explainability. As artificial intelligence systems progress, they're expanding use in high-risk sectors like healthcare introduces potential risks to public safety. This concern primarily stems from the design of certain large language models. Many sophisticated AI models function as intricate black boxes, concealing their internal mechanisms, and rendering it difficult for users or even their models developers to unravel the decision making process. While these models may excel in low-risk applications such as providing product or movie recommendations, deploying black box AI systems in high-stakes sectors warns intense scrutiny. Without transparency into how key choices are made, ensuring full accountability when costly errors arise becomes vastly more difficult. Similarly, unraveling complex lines of responsibility across multiple stakeholder groups grows immensely more challenging. Thus, for AI systems which could pose risks to human life and liberty, explainability should be non-negotiable. For instance, while AI used in clinical settings can improve the ability of clinicians to diagnose and treat patients, without proper transparency requirements, these technologies can also dangerously distort medical diagnoses and recommend inadvisable treatment plans. If the model is operating with a black box, while meaning clinicians may follow this misguided counsel, result in patient harm. If an explainable AI model were to be used in this setting, the AI system could explain the factors behind its treatment plan, enabling the doctor to catch an error that could have been introduced by a faulty data set or misrepresentation of the fact pattern. The insights provided by explainable AI ensure that a human always has the necessary information to intervene where necessary to prevent a harmful outcome. As AI permeates across sensitive public spheres, lawmakers have an obligation to future-proof policies by establishing transparency requirements. For instance, in high stakes contexts like healthcare, where AI could guide pivotal human decisions, legislators should implement standards requiring globally explainable models. Global explainability requires the model to be able to produce the entire causal chain of data sources, feature transformations, model architectures, and inference pathways shaping each result. By mandating total traceability into the complete machine learning pipeline from raw data ingestion to decision output, global explainability supports external auditing and accountability when necessary. For AI systems that could impact the physical well-being of an individual or their liberty, no lesser standard suffices. In lower risk settings like supply chain optimizations, locally explainable models contextualizing a model's recommendation may adequately balance innovation, efficiency, and transparency. Local explainability provides visibility into the rationale behind specific AI generated decisions without exposing the entire system mechanics. For example, AI systems used to enhance supply chains may highlight the demand and supply factors motivating a suggested shipment re-root without having to recall all the data that led to such a recommendation. Such limited insights allow users to interpret recommendations responsibly without inhibiting commercialization or requiring total disclosure which can be needlessly burdensome in certain contexts. In settings where AI merely assists or augments in consequential human decisions such as entertainment recommendations, rigid explainability mandates often prove excessive and risk-impeating innovation. For research and development as well, avoiding blanket transparency protocols helps catalyze breakthroughs across exploratory work where causes and effects remain unclear. When crafting policies, lawmakers should resist one-size-fits-all standards ill-suited to the nuances of AI application. Instead, tailored transparency guardrails align to specific use cases balance oversight with advancement. Practical oversight assurances can nurture an AI ecosystem where both security and scientific development thrive in balance rather than in opposition. The path forward resides not in reactionary constraints but adaptable guidelines, cementing protections where most warranted without diffusing innovation energies across less sensitive domains. With conscientious governance tuned to context, AI can progress responsibly towards the heights of its potential. If you turn your attention to the PowerPoint, there's a loose representation of what explainability in real world environment would actually look like. If you look to the top, you'll see the black box model input, synthesization, coming to a conclusion and then it shoots out the output. What's unique about this situation is that output, what led to it, a show your work type anecdote from a math class doesn't exist. There isn't the work to the long division problem. It just gives you the answer. Under an explainable framework, as you see at the bottom, there's another step in the process. There is that presentation of work, the background information, and obviously it differs as I laid out between local and global. But at the very least, it gives you the necessary information to feel comfortable with the response. It gives that user in a high stakes environment the ability to interject if something negative could happen as a result of automation or using one of these systems. Which leads me to the way forward. The advancement of artificial intelligence presents an exciting opportunity to propel George's economy into the future. With common sense policies centered around transparency, accountability, and data privacy, Georgia can set itself apart as a leader in AI governance. By adopting a proactive, principled approach, lawmakers can craft balanced regulations allowing businesses to deploy AI safely without smothering innovation. If Georgia chooses to boldly embrace AI's rise, rather than resist the technology, the state will have a chance to enter a prosperous new era. While we are still at the earliest chapters of the AI revolution, this state has an opportunity to cement an enduring foundation for the future that can guide responsible development and deployment of these systems. I don't know what happened there, but the path forward holds many challenges. But with pragmatic regulations balancing oversight and security, with conditions suitable for rapid innovation, Georgia has the chance to become an even better place to live. Thank you all again for your time and for granting me the opportunity to share my perspective on these emerging policy challenges. Happy to answer any questions you may have. Thank you, Jake. I appreciate that was very informative. You've been an expert in talking to a lot of leaders about this and you've gone around the country and obviously been on the news about it. Help us understand from your perspective, is there any type of a theme of legislation you're seeing from any one at the state level or you're seeing anything in the local or federal level at this point? Well, I think the approach to legislation has essentially taken two shapes. You have a reactionary policy that is following or chasing the item of the week, a news story about a data leak, for instance, or a story of a patient getting harmed by an AI model. And there's reactive legislation that aims to capture the emotion around that event and push out a rush decision. And generally from my experience, that is a very misguided approach. It often blurs the application element of this where you're reacting to something that happened in health care. And suddenly you have a spillover effect and finance and all these other industries that didn't need that type of regulation. And so those usually die on arrival. And then there's another side of AI application for legislation or strategy rather that's more foundational. It's pursuing more pillars of good policy, whether it be transparency, privacy, or accountability. These policies are kind of slow to progress. It takes a lot of conversations because while these are bipartisan terms, not everyone is up to speed necessarily on what it means in practice. And a lot of partisan bickering occurs over whether transparency benefits a particular side, but universally what happens is it benefits the constituent and the consumer. It's not necessarily a partisan victory for a particular side if one of these policies advances. This is one of the uniquely bipartisan arenas. And in the federal domain in particular, in my experience, the lawmakers that have the best grasp on this legislation are working in conjunction with each other. They're bipartisan bills and they're built around foundational computer science principles rather than a news headline that they read in the morning on the way into work. So I'd say those are the two different approaches. And unfortunately, there isn't necessarily a model at this moment because it's essentially a tug of war. People are looking for different paths forward, but it shows there's a clear opportunity here to present that model to be the blueprint. And another question regarding the disclosures, I think that's going to be a critical problem that we're going to face in the future, whether it be a photograph or just someone speaking an audio or certainly in a video. As far as that's concerned, obviously you reference political and of course, you know, everybody here had to run a campaign to get into elected office. But this certainly transcends all politics into almost anything that we're looking and seeing in today's day and age. Are you seeing any of that coming from I would call it more traditional industry where they are marking their own commercials or videos or have we seen anything on an industrial espionage type level yet on that? You're beginning to see companies essentially self-regulate here. They're putting in the disclosure or the watermark that alerts the consumer that an image they're seeing is at least partially synthetically generated. But it's not universal and there are certainly instances you can jump on Yelp and scroll through cafes that have AI generated cups of coffee that are their marketable product for a consumer that is trying to seek, you know, what coffee shop should I visit. And that's really where my concern arises. I think the big issue here is the information imbalance. The business knows that that image is AI generated and the customers of the belief that it was taken in the coffee shop or in the studio representative of their product. And so I'd say the vast majority of instances are not being given a watermark. They're not being given a disclosure. And, you know, kudos to the industries and the businesses that are choosing to proceed that way. But it is not necessarily a self-governance situation. It's not something we can just allow for, you know, self-regulation to solve for. Because there is real monetary harm, there's emotional harm, there's all sorts of downstream effects to the average person that can arise because of this subtle manipulation. On a state level, it isn't necessarily clear that we've seen, especially in an electoral context, instances of these kind of synthetic ads, these deep fakes necessarily in this most recent cycle. But if you look to Turkey and its last election cycle, which occurred over the summer, they were everywhere. You had candidates explicitly stating that they dropped out of the race due to these deep fakes that were being launched against them. Everything from pornographic videos to instances of them celebrating with terrorists. This happened at scale and it was bolstered through other digital vulnerabilities that we had such as the prevalence of social media bots, right? I mean, you make a deep fake and then your issue is essentially, how do I get people to see it? Well, if you have a network of social media bots, you can boost it yourself and suddenly everyone's getting it on their feed and it's everywhere. And those types of parrions where it is the new technology.\n\nAI generated content or sophistication, these models that enhance our ability to deceive and really disproduce content paired with another tool, and that's where the real issue arises. And you'll never solve for the nation-state actor. But you can make sure that these campaigns operating in the United States comply to some form of universal standards such as disclosures or watermarks. Thank you. Questions from Queenie members? Chairman Thomas. Thank you, Susan. Thanks for presenting. Good to see you, Jake. Yeah, good to see you. So one of the biggest questions that I've been getting is, how do you define AI? Yeah, it's a tough one because it really sets the legislation up for success or failure. Right? That's how everybody starts with defining what AI is. The first page, really. And so there are two philosophies. You'll see a more targeted computer science-y kind of linguistic composition where they get really technical and it differs build-to-build. It can be something that's targeted towards, you know, a particular type of algorithm or something of that nature. But I generally prefer to go towards the broad and go towards essentially a computational system which can automate a task or something that would have required human cognition, right? Something that typically would have required an own person essentially to complete the task. And those are overly broad and don't work universally. I think that's the part I get into in terms of application is your definition will vary depending on what you're trying to do. But I think that really is a good kind of general understanding. The person reading the bill, everyone gets it. That if the AI model can essentially take over the task for a person, then it's AI. And it's the most elementary kind of dumbed-down version possible, but it gives you a good starting point. And then you can kind of scale it up from there depending on the application. I actually started pulling definitions from several different locations. One was National Artificial Intelligence Initiative Act of 2020, BSA, the software alliance. And I actually even went to Google and then I actually asked ChatGPT to define it too. I did it, actually did a pretty good job, in my opinion. And what I found is there seems to be a common occurrence of certain words, algorithm function, those type of, you know, in the computer science language, those are, you know, definitive things. And I kind of continue to dig and I got into the American Data Privacy and Protection Act and it uses the term covered algorithm. Are you familiar with that being used across the spectrum? Yeah, the covered algorithm is essentially referring to the targeted scope of the legislation. And so what it's referring to there is another definition of algorithm somewhere else trying to kind of give the scope of the bill. And so it's not necessarily like a technical term for a type of algorithm, but rather giving context to the coverage or the scope of authority for the piece of legislation. Right. So because we're not too big on technical terms, but legal terms, we kind of have to hammer out around here. So my other question, I've got several, I could go on for a while, but I'll try and limit it. You talked about basically what I call a silver bullet approach. You're not going to be able to take this all out in a silver bullet, right? No. So it sounds like to me what I'm hearing is you almost need to classify it based off of a risk, where maybe you have a risk where something's life safety, you know, I say liberty. So freedom of speech, elections, that tier, maybe more of a non-critical or non-hazardist and maybe unregulated. Is that like a framework that you've seen being used? Yes. So obviously in the states where at a very early stage in terms of AI legislation and one of the few things that I would probably give the European Union credit for is that risk scale. I think that was probably the one few, one item in the EU AI Act that actually kind of made sense. And it is essentially a pyramid of risk. Starting at the very foundation level, you have your in-consequential systems like a Siri or an Alexa, these kind of worthless pieces of technology really. That ring you some type of service. But at the very top, you have that kind of life and liberty element. Things that could result in death, could result in bodily harm, things of that nature. And they have a very high level of scrutiny applied to them in order to essentially be in compliance with the bill. And so I don't think that you could truly copy and paste the EU AI Act's risk scale. They value risk very differently. But that model of what they kind of built out could be tinkered with to a degree where it makes sense in Georgia too. I guess this is my last question. During my research, I found that it looks like Texas did some legislation regarding gender to AI. Are you familiar with that legislation at all? Yeah. So there is a piece of legislation that they passed several years ago, in particular for a electoral context. And I forget the exact cut-off date. But it essentially bars campaigns, political action committees from generating AI generated advertising about 90 days out from the election as I believe the timeline. And so this is another route you could go rather than watermarking or disclosures is allow it to happen rampantly but drop the hammer after a certain period of time or within a particular window to, you know, it's a high risk types of technologies. You also have your high risk timeframes, these types of high in areas of sensitivity. I think that if you're looking to model legislation, it is a pretty solid starting point. I think they did a lot of good things. And I think it's worked generally. So that is a good starting point for a disclosure type bill. Other questions? Yes, Senator Robertson. Yes, thank you for coming in today and talking and looking at the EU model. And listen, I yield to the expert on this. Unexceptible risk under their matrix. Cognitive behavioral manipulation of people or specific vulnerable groups, for example, voice activated toys that encouraged dangerous behavior to children, things of that nature. Social scoring, classifying people based on behavior, socioeconomic status or personal characteristics. Doesn't Siri do that today? And what regard do you? Listen, I've got these dots all over my house. So I don't talk in my house. Yeah. My daughter level and my wife loves them. I go hide in my truck. But basically, you see the pattern as I live in that environment. Sure. That everything, every ad, every is based on what that technology determines to be my socioeconomic status. Yes, I understand. My behavior, absolutely. Yeah. And other things. So, you know, if we look at these and I'm not a lawyer, I'm somebody that enforces laws. And so for me, it needs to be very clear. So it seems like that we're going to run through a whole lot of surgical manipulation of the words that we have to use up here to make sure we're not throwing a casnet out while we're trying to surgically create legislation that protects our citizens from these types of this type of manipulation. Yeah. So I believe what the EU is trying to get at in that particular section was actually a social credit system similar to that of what you would see in the CCP. Right? So using an AI system to give you a score that would then potentially bar you from participating in a particular element of your data to life. What you're describing is something that you could solve with a privacy by design model of legislation, right? You're pushing these companies to value privacy at the very foundational level, earliest stages of the technology and its development, things like the live microphone informing an ad. It's not necessarily due to your socioeconomic status, but it's doing something on the back end there where to synthesize the information that you put in and putting you within a market bracket to push an ad. What the UAI Act is essentially barring there is when it synthesizes that information and puts you within a bracket, it could then potentially restrict you from participating in a particular way of life. I think if you were looking at it would restrict you, the government would restrict you. Not the manufacturers, right? Well, so it could be the manufacturers of focus, right? It could be the government, but what is also essentially covered in that type of legislation or law within the EU would also be a company potentially limiting you as a user from a particular feature of the application. So let's say that you typed in something that put you on a particular pathway and your credit points that were being assigned by the AI system were to basically say that you fall in out of favor with the company, it could potentially then restrict your access to the platform. I think that is the hypothetical scenario that they were trying to venture down. But more importantly, I think it is the social credit system to solve for your kind of privacy concern there for targeted advertising, things of that nature, enhanced by these models. What you can really push for is anonymize data, data retention policies, things like requiring the deletion of certain types of data collected by certain models after a given period of time, call the Sunset Clause, something. It collects your birthday when you make an account on the platform, but it does not need to have that information for the entire time you use that. It's only to verify at the beginning stage. And so what the EU had done in another piece of legislation or law, the GDPR, was essentially put data minimization things in motion. And so what that does is it requires the deletion of certain pieces of your profile that aren't necessary to the core functions of the application. And that is something that is very easily done, hasn't really been done anywhere in the US. And it's as simple as, you know, if it isn't necessary to run the app, if it isn't necessary for the core features, why are we allowing these companies to hold onto it and use it against us for years at a time? And that is what enables that manipulation. And when you consider it in an AI context, it could lead to a generative ad designed specifically towards you that pushes you down a product tree, right? Like you're now purchasing things because it was perfectly targeted to you. Those are the types of data minimization principles that would be very beneficial for a consumer or a constituent. Thank you. Other questions? Chairman Payne. Earlier you were talking about reference as you were referring to making something user-friendly for elderly. I mean, as far as the technology making something... I did not explicitly mention that, but I could see where the... Sure. People are not technology. Yeah, yeah, certainly. Could you define user-friendly? I don't know. It's not as so that I was getting asked by a apologize if that was what you took away. I think in terms of user-friendliness, the best pathway would probably be that explainability model simply because it drops that information barrier, right? I mean, it allows for the user to now get what is happening behind the scenes inside that black box. If you as a user, kind of you a product as inaccessible or as something that is hard to comprehend, a lot of that has to do with its inner mechanisms. Why does search result produce certain things? These are things that you're not really able to come to a conclusion of. So I think an AI model that is user-friendly would be one that is explainable. It's one that you could ask you a question and it could tell you why it came to that result. When it comes to making features themselves accessible, it's a little more difficult to define such a thing. You know, interface design is something that's very complex and everyone has a different approach. But I think as a technical feature, something that is explainable is inherently accessible. Okay, thank you. All right, we're going to do one final question from Chairman Thomas and we're going to then move on. Thank you, Jake. My last question is obviously it's our job to set the next generation up for success. That's pretty much one of the first things I think of when it comes to policy making. And what are you seeing across the country in investment for, you know, our K-12 and our college students? Is there any good legislation you've seen out there to? I honestly have not seen anything that I would point to as like this is the gold standard. Well, we're working on that. But so I was just curious to get you to point me in a direction. It sounds like right now it's yeah, so to be us. There are a few pieces of federal legislation that are in circulation. I'm forgetting the names, which I'd be happy to send you afterwards that establish essentially like a federal grant program. But at a state level, I think you would go much different route. It could just at least kind of show you where they're deploying the words they're using. But I'd be happy to share after the fact. There's this is the last last question from not real quick. I'm not sure if I was going to Thomas. My alma mater has put together an 85 million dollar facility that every student no matter what their major is will be going through at least one semester of AI education. I think that's a great basis at least in terms of from an educational standpoint, but I agree with you. We need to have policy around that in rules and regs. I just wanted to mention my alma mater. So thank you. You didn't mention who your alma mater is. There's a reason why he didn't mention who his alma mater is. My alma mater is the University of Florida. Strike the F and the official record boys. Thank you. Jake, thank you for your time today, but also numerous follow ups, which you'll be hearing from many of us as we move forward. So when you go before another state sometime in the future, you can say we know the gold standard and it is the state of Georgia. Absolutely. Thanks for having me. Appreciate it. And just for everyone's sake to let you know in the first committee meeting that we had held on the Senate's side, we heard from Dentons Offer and the World's Largest Offer. We heard from CGI, which you might think members, Cap Jam and I before their name change. And we heard from Amazon as the beginning processes before we got here. And before we bring up our next speaker, I want to just mention a couple of things. First, I want to thank Ms. Becky Stone who's in the back who is helping us Alice and I and everybody else and pulling the other she is, pulling a lot of important information together for us. And this is a word of warning, but a takeaway for everyone. If you do not have a family code word, you need to have one. Don't ever email it and don't ever text it, but make sure everyone in your family knows it knows it well. Because as has been explained by several people, you're going to get something that's going to seem like a facetime call. And it could be from your son, daughter, parent, grandparent, and uncle next door neighbor. It will come from them. It will look like them. It will sound like them. And it could be somebody with a gun on their head saying, if you don't send them $10,000 in the next two minutes, they're going to kill them. And in the time you're even fearful of hanging up the phone and trying to call them back, which is what you need to do. Having a family code work resolve some of that. So this is one of the many tips that people can do. It doesn't cost anything, but it will hopefully protect you. And a lot of people are still, you know, they're excited about all this need technology. And if you've not been using chat, GPD or a clot or a bar or all the other ones that are out there and I've used probably a dozen of them now, it's important that you get a little familiar with how those work. Some of it very good. I as an example asked it to write an 800 word article for me back earlier this year on what pre session or sorry, recession would look like for a state government based in the Southeast United States and what actions could be taken. And in five seconds it wrote me an exact 800 word article and I didn't change a word. It was so good. And there are ways that you can now mark those. How much of that was used and developed by AI versus not. But these are good things for each one of us as we have to become educated to do this stuff. Now here is the negative. You may have seen that in some cases it developed its own case law that did not exist. And then a lawyers cited it as case law. They got in a whole lot of trouble. It created a Bible verse that did not exist. And one of my most favorite personal experiences, one of them called which I use all the time. And I like that tool particularly because I can upload a 200 page Adobe document, PDF and say summarize this. And in five seconds it'll send me two pages summary notes. Those things are great. But I thought well tell me about myself. What gave me a long biography of myself. Most of it was completely untrue. Said I was a former state representative and I went to college certain places. I was certain committees. I live certain places. Almost all of it was inaccurate. So I asked him what was it source. What was your source of information? It's so I had many sources. So I corrected it. It took all my corrections. But it could have taken anybody's. Think about that for a moment. Right. So these are the things that we will struggle with a little bit. And this is a beginning of a very long process for us. What we work on is a combined house senate and the governor's office. Now we will have a bill in 2024, 25, 26, 27 and so on. As we continue to do things that are right. Again the parameters I think are important. We can't be too surgical addressing the issue of the day as mentioned earlier. So there's nobody better next that come up than probably one of the largest most successful impactful technology companies in the world. Which we all use daily which is Microsoft and Ryan Harkins. If you would please come up to the podium. And if you want to introduce anybody with you, please feel free to do so. Good afternoon Mr. Chairman and members of the committee. My name is Ryan Harkins. I am Senior Director of Public Policy with Microsoft and I'd like to thank you for having me today. So without question, we at Microsoft are very bullish and optimistic about this new range of technologies that we are calling AI. Our CEO Satyana Dela has said that this may be the most fundamental piece of technology ever created and there are all sorts of potential benefits that AI could bring to solve problems that are burdening society or businesses or governments or individuals. But if I look at the outset, it might be helpful to take a step back and examine why are we hearing about all of this now? What's changed? What developments have led to the creation of this new wave of generative AI? And what is this thing we're talking about? Because as the Chairman mentioned at the very beginning of the hearing, the term AI has been around for a long time. It's been around for decades but it's sort of been thrown around loosely as a marketing term. It's come to be used to refer to anything that might be automated. When banks came out with ATMs in the 70s, that was called AI. Automated or checkerless checkout lanes, that's called AI. So when you hear people say AI, they could be referring to anything like relatively simple programs like your anti-spam filter or the autocorrect feature in search engines to really big, sophisticated, large language models like GPT-4. So what is AI and what has led us to this place? There are really at a high level three developments that have led to the current and new generation of AI technology. The first is the development of new and sophisticated machine learning algorithms. These are computer programs that can learn to recognize patterns from large data sets and to get better at deriving conclusions or inferring things from those data sets over time. And we saw a whole wave of algorithms in machine learning programs that could for instance process unstructured data. So whereas as anyone heard of SQL technology, SQL server structure query language, that's sort of a traditional database technology. In order for SQL to function appropriately and process data, the data has to be formatted first. So think big Excel spreadsheets full of tables of data. And it's really good at things like if you're a bank, it's kind of important for you to be able to determine how much money is in a particular count at a particular moment in time. But actually going through the work to structure the data first can be a lot of work and it limits what you can do. So we came out with new algorithms that could process unstructured data. So no matter whether it was YouTube videos or tweets or emails or what have you, it enabled us to process a lot more data more quickly. We then as anyone you've heard of transformer architecture and no not the cartoon, that's the T in GPT. These are new algorithms that in short enable the development of large language models much more quickly than would have been possible before. So new and sophisticated algorithms. The second is data. AI depends on lots and lots of data. And for years you could pick up a newspaper or a magazine and someone would have some new creative metaphor just to illustrate how much data we are now creating. This is my latest favorite because it not only illustrates how much data we are creating and storing but how cheap it's gotten. So in 1987 it would have cost you over a billion US dollars to store every written work that had ever been produced in the history of humankind. Today you can do that for less than a grand. Pretty soon you'll be able to do that for a few hundred dollars and it is our access to incredibly large data sets as much as anything else which is powering the development of large language models and other new AI technologies. The last piece is processing power, massive processing power. In the 1970s computer scientists had developed central processing units that could process something on the order of 92,000 instructions per second. That sounds like a lot until you consider that every one of us is carrying a supercomputer in our pockets that can process billions of instructions per second. When you combine that into massive data centers the kinds of data centers that are required to train large language models today. So Microsoft for instance outside of Des Moines, Iowa built a supercomputing data center solely to train the large language models that open AI the company behind GPT and GPT4. We built this data center. It's massive. It has over 285,000 central processing units. It has over 10,000 advanced graphics processing units. Each one of those has network connectivity of 400 gigabits per second. So the amount of processing power that we can bring to bear on any problem is leaps and bounds beyond where it was even a few years ago. So you put all that together and it has enabled the creation of essentially a new form of computer program. And it's I think important to take a moment to illustrate just how different these programs are and how different they are in functioning from sort of traditional computer programs. So has anyone ever heard of the Russian English translation project? And for the record I have never had someone say yes you would you would have stunned me if you had you should have. So in the 1950s linguists from Georgetown University and computer scientists from IBM got together and they decided to try and build a computer program that could translate Russian into English and vice versa in real time. And so what did they do? They took the dictionaries that will cab you away from each language they program that into a computer. And then they went through and pain stakingly went through and tried to manually program in rules to dictate when a particular word of sentence should be translated into something else if this then that. And it turns out their translations were pretty uniformly terrible. And they were terrible not just because I'm told Russian is a very difficult language to learn but because in addition to just the vocabulary you have grammatical rules. You have exceptions to grammatical rules English is notorious for having a lot of those more importantly you have context and getting all of that right is really hard. So you fast forward three decades and IBM says you know what what if instead we decide to use math? Rather than trying to manually program in rules to dictate when something should be translated into something else we're going to use math to try and infer the likelihood that the next word in a sentence should be x, y or z. Turns out they were on to something. Their translations improved dramatically. You fast forward ten more years and they said you know what would really help is if we had a large data set of high quality translations that we could feed through our program so that it could learn that is the learning in machine learning it could get better over time at calculating the likelihood that the next word should be x, y or z. Where could we find that? Well turns out Canada is required by law to maintain all of their parliamentary transcripts in both English and French. They had ten years worth of transcripts three million sentence pairs. They ran that through their program they were on to something their translations improved dramatically. You fast forward about ten more years and Google comes along and Google says what if we could take everything we could find on the internet? Trillions of words. We won't know whether any individual translation is high quality or necessarily all that accurate but perhaps just the sheer volume of data that we can run through our program will improve things and they were right. Their translations improved leaps in bounds and it was those sorts of techniques that led a team at Microsoft Research in 2015 to achieve for the first time statistical parity with human translators. In those sorts of techniques it is that process that fundamentally undergirds large language models and what models like GPT4 do today. What they do is math. If you had a sentence I woke up this morning and there was a beautiful blue blank. What should the next word in that sentence be? Sky, bird, butterfly, something else. These sorts of models they are a probability distribution across words and so they can infer or calculate the likelihood that the next word in that sentence should be sky, bird or something else. Once you can start to do that you can say what is the likelihood or what should the next sentence look like? Once you can start to do that you can say all right what might a paragraph look like? What might a short story look like? But it is fundamentally all based upon math in firm the likelihood\n\nand getting better at predicting over time what that language should look like. Now, it's important to note that because of this there are things that these models currently can't do. So for instance, these models are predicting output based upon relationships they have learned from through large data sets. They still cannot provide causal explanations of things in the way that humans can. But they can accomplish things which are fairly powerful. This is my favorite anecdote because I was once an aspiring young lawyer and if you told me ahead of time right now we can guarantee you 90th percentile in the bar exam I would have taken it hands down. Apparently GPT-4 can pass the bar exam in the 90th percentile. And we are really excited about this because we see the potential to use AI to augment human abilities. And this is the way we think about technology in general at Microsoft. In generative AI programs which can produce or generate new content does in fact promise all sorts of benefits. So for example the chairman mentioned content generation. It can do a lot of the work for you to spit out an email or write some other summary that you want to write and then allow you to simply edit it to make sure it's accurate and send it out. Summarization. We are seeing government agencies for instance across the country consider the use of technology like this to provide bill summaries or reports to summarize more complex or lengthy contracts or policies to make them more accessible for people. Co-generation it's been estimated that 40% of the computer code that is written today is produced using the assistance of AI. This could make the work of whether it is resolving cybersecurity vulnerabilities or transitioning arcane out of date code. Perhaps state has a legacy databases or systems that need to be updated. This could make that work more efficient. And in addition to that what's called on the slide semantic search. Or the ability to enable constituents and others to more easily access government services. So this is an example from New York City. New York has made something called my chat bot live which a constituent can use. This is an example of say a small business owner who wants to know what they need to do in order to open a restaurant in New York City. And there are all sorts of red tape and rules and things that you would need to do with in order to do that. Well this can make it easier for someone to know just what it is they need to do. What are the permits you need? What are the things I should be thinking about? And you can interact with the chat bot using natural language in real time to provide information to people more quickly, more easily in a way which is more that is more accessible and more relevant to them. We've also seen I mean there are all sorts of examples that we could run through. This is one that I think is important. You know Microsoft working in conjunction with some other partners including the University of Washington is using AI in high powered satellite imagery in order to assist first responders in responding to natural disasters. This sort of technology has been used both in the wake of the earthquake in Turkey. As well as in the fires in Lahaina in the island of Maui in order to try and identify more quickly where the destruction has occurred which buildings have been destroyed. So that first responders and others can get out in the field and start doing their work more quickly. But we could spend the entire hearing cheerleading AI and talking about all the proposed benefits. We're also clear right at Microsoft that there are very real risks of harm that we and policymakers and constituents ought to be concerned with AI after all is a tool. And as our president Brad Smith has written any tool throughout human history can potentially be used for bad or for good. So how do we ensure what kinds of policies should we have in place to facilitate provision of the benefits of AI while taking very seriously the real risks of harm. We at Microsoft have been thinking hard about this for some time. So in 2016 our CEO Satya Nadella wrote an article in which he outlined a set of ethical principles that in his view ought to govern the development, the deployment and the use of AI. Those principles were subsequently adopted by our company and we have spent the last six, seven years building out infrastructure in our company to make those principles real. So we not only have these six ethical principles and these are them up on the screen things like fairness. Ensuring that AI programs won't treat similarly situated people across communities differently. Principle like reliability and safety ensuring that AI programs will be used safely and won't pose unreasonable risks of harm to individuals, particularly if they are going to be used in scenarios where they could hurt people think autonomous vehicles. Privacy and security ensuring that these programs will abide by existing laws relating to the use of data among other things. Inclusiveness ensuring that perspectives from all of our communities across society will be taken into consideration when this technology is developed and used among other things. So at Microsoft we not only have adopted these principles but for instance we created essentially an ethics committee. We are calling the ether committee. We now have the responsible AI council it consists of some of the senior most leaders from across the company and they are responsible for overseeing the way in which we develop AI the way we deploy it. And for sometimes reviewing potentially sensitive use cases this council has turned down business at times over the years when we faced proposals to deploy AI technologies facial recognition for instance in some ways in which we thought it would be unsafe or unethical to do so. We've created an office of responsible AI with a chief responsible AI officer and this team among other things is responsible for taking those principles and turning them into concrete rules engineering standards. Our responsible AI standard this is a standard that we've made public and it contains rules that our engineering teams are required to follow when they are developing AI technologies so that. Those ethical principles inform the way we develop our technologies from the ground up so they're not just window dressing they're not just after thoughts. But it's all well and good for Microsoft and for other industry players to do what we think the right thing is to self regulate. We need more. This is a space where we and others recognize we need laws. And there's a very robust conversation that has been happening here in the United States and frankly around the world about what that might look like. Last spring our president Brad Smith released a white paper which detailed some of our ideas a five point blueprint for ideas policy makers could consider in this space what might regulation look like. These were just ideas. They were intended to further or propel the conversation and we've been happy to see the conversation continue to proceed and unfold. So we've seen the White House over the summer issue a number of voluntary commitments that we and most others in the technology industry have agreed to abide by we've seen a number of executive orders being issued across the country. Not just from Washington DC but from governor's administrations to encourage state agencies and departments. To examine the range of potential benefits how can AI be used to improve government services to make government more efficient what are the risks of harm we should be concerned with. What kinds of rules might we might we want to see and. From Microsoft perspective. We think it is important to be clear and for policymakers to be clear when they are thinking about what might the rules look like. To be clear about what part of the technology stack are we talking about because there are different components that go into making things like large, large language models. And it may be that different rules might be more appropriate for different parts of that stack so. At the bottom of the stack if you will is the infrastructure that is required to train large language models infrastructure like. The super computing data center we built in Iowa that I mentioned. There are very few institutions or entities on the planet right now who have the resources to build that type of infrastructure. On top of that you can take the advanced algorithms or machine learning software in order to then train powerful. Pre-trained models GPT for. And it may be in fact we've seen conversations in Washington DC. About whether there ought to be some form of licensing regime. Issued by the federal government. Around the construction of the infrastructure or the models. And that the theory behind that is that it is important and necessary for the government and the public to have some assurance that if people are going to either build the infrastructure. Or the models we have assurances that they will incorporate in a buy-by certain basic safety protocols and rules. That they will know who they're building these models for. So that for example we can know these models won't be built for a terrorist organization or for an adversary or someone to build a dirty bomb or engage in some other form of nefarious activity. We've also seen a lot of conversations in particular in the states at the top of the stack. And at the top of the stack you have applications. You can build developers can build applications. So if you've used chat GPT that is a chat application that uses application programming interfaces or APIs which you can think of as hooks. That allow an application to leverage the capabilities of the model. So the new Bing for instance because we don't Google in my household we Bing things. Just saying. The new Bing utilizes the capabilities of GPT4 to provide you with we think are more relevant search results. And that is the layer at the top of the stack where a lot of the risks of harm people are starting to identify and discuss arise. Because that's where the public government businesses etc. are going to be interacting with and interfacing with these powerful models. That's where a lot of risks of discrimination or other things may arise. And so we are starting to see state policy makers put forward ideas in this space. And frankly we're at the very beginning of this conversation. I think most policy makers are still trying to get their arms around. What is this thing that we're calling AI? What is it? What does it do? How does it work? Where are the benefits we should be excited about? Where are the things we should be worried about? But we are also seeing some proposals to write rules. The first thing that we say every time we go into a state capital or frankly in DC but I mean good lord do we expect them to do anything? It was rhetorical. Pass a comprehensive privacy law. If you do not have a comprehensive privacy law you should pass one. AI relies upon data. You can't have responsible and ethical AI without rules about data. What's privacy? It's rules about data. We at Microsoft have been calling for a comprehensive federal privacy law since 2005. We're still waiting. We've been heartened to see states starting to act however. Thirteen states have now passed comprehensive privacy legislation in this country. The states are not simply including the states of Tennessee, Texas, Florida, Indiana, Montana, Virginia, among others. Beyond that we've started to see other proposals. We are seeing states consider not just task forces and whenever you mention task forces you kind of can feel people's eyes rolling. Sometimes where issues go to die. In this instance I actually think task forces can be incredibly helpful and a helpful part of the conversation. Not only to educate policymakers about this space but also frankly to identify there are all sorts of existing laws on the books that should and will continue to apply to the use of AI in a number of different scenarios. We've had civil rights laws on the books for decades. If AI is going to be used in ways that would discriminate against people in the provision of housing, credit, insurance, etc. those laws would still apply. We have consumer protection laws on the books. Those laws would still apply. But there's still work to be done in my view for policymakers to be crystal clear how existing laws would apply and where the gaps, where the gaps that they should fill in. And finally we've seen other efforts. We've seen legislation proposed in order to require risk assessments for instance. If automated decision systems are going to be used in order to make consequential decisions about people, decisions like. Whether to provide you with education, employment, housing, etc. We've also seen some early efforts including in our home state of Washington to try to address the rise of deepfakes particularly as they may impact or influence elections in our political system. State of Washington this past spring passed the law outlawing the use of deepfakes and political ads. It is something that we're now seeing other state policymakers take up and consider seriously. So with that I thank you Mr. Chairman and thank you again for the opportunity to be here and I'll stop there and I'd be happy to try and answer any questions. Ryan thank you that was excellent. I think it provided a great foundation for the committee members. And then certainly the recommendations are well heard. We are working on a privacy law for Georgia. We are behind the eight ball and that I think you mentioned this. I wish that our federal counterparts could solve that so we don't have 50 different privacy laws but undoubtedly that's where we're heading. I want to get back to that New York City model that you brought up which I thought was quite interesting and I'm thinking of the applicability of not just protection and guardrails that part but the benefit to our state government for someone who is trying to navigate a business license or unemployment or social service or starting a business. Things of that nature I think are really beneficial to us. Is that something that they are using kind of off the shelf you know large scale models to do that work are they buying something or is that something that they are building themselves in turn away I'm curious. It's something that so you can take a large pre trained model and then have it tweaked or trained further for a specific purpose. And yes all of the examples that you just rattled off those are all areas which are right for this kind of innovation. So you can take a pre trained model and then you can have it trained or tweaked further for your discrete purpose. And that may even help resolve some of the concerns that we've seen from say consumer uses of certain chat bots and their likelihood to I don't know why computer scientists come up with terms like this to hallucinate in other words say things that are just are not true. So yes. That is the term they use hallucinate hallucinate. I'll loosen it. Yes. And last question before we have other committee members chime in the standards and the rules which Microsoft has done proactively obviously for for the company right and probably those rules will propagate hope with some of your clients. These are some of the things that we're going to need to do in a state level from from probably a two-pronged approach one is for the way our state operates as well as you know how we then do something in the law to again put those guard rails in place without stifling innovation. I'm going to ask proactively asked the question that chairman Thomas did earlier are you aware of any legislation laws rules out there that you can point to as that gold standard. I would not point to anything today as a gold standard I think that the conversation is still too new and too young. There are things that we can't point to though that we think are positive and are worth considering as part of the conversation. So NIST has put out a risk management framework for AI with rules that are intended to help organizations implement and adopt basic governance around the development and use of AI technologies. We have already committed that we abide by the NIST AI risk management framework. We think it would be smart for other institutions and entities to do the same and that is something that state governments could consider. Does it make sense to say for all of our agencies and departments to adopt the principles at the heart of the NIST AI risk management framework. Does it make sense to update your procurement rules or procedures to ensure that any vendors that are going to be providing AI technologies to the state abide by the sort of basic governance procedures among other things. Excellent. I know there's probably some other questions for committee members. Senator Robertson number eight. Yes, thank you. Now you said the NIST standard, right? What is NIST's purpose? What are they? So NIST is a part of a federal agency or part of a federal department. And its purpose is to help among other things put out standards and other governance for the scientific community, for different industries. And so it is an area where in this space for instance, we think they've done a very good job of outlining the sort of procedures and rules which just provide good smart basic governance for entities to follow. I'm a little shocked who said government organization would look for good governance and sound mindness and that's kind of where I'm going. It's almost like we have government competing with the public sector competing with the private sector, which the private sector of course is driven by shareholder. And the government sector as we've seen of late is driven by partisan policies, which I think is is a major concern to everyone involved because of this distrust that we have for government. And especially when it comes to the information that we're talking about feeding into into this. And so is this who we have to depend on to build that perfect as close as possible ethical foundation that we need as we build the tower of AI? Is there a third? I wouldn't necessarily say that we have to rely upon NIST for all of this. But the standard they have put out from our vantage point has not been sort of infected with like the kind of hyper polarization that we've seen in politics and in other parts of the government. I think the standard they have put out is consistent with basic good governance rules and in the kinds of approaches to these sorts of problems that industry has been implementing for decades now in the privacy and data protection space. And industry broadly has recognized that it is in its own interest to have good governance and basic structures and rules in place. So, whereas 40 years ago maybe none of this existed now. So, the company that deals in data that is of a reasonable size will have a chief privacy officer. They will have a comprehensive privacy or data protection program so that the company can assure its customers and its clients that it is doing the right thing by their data. That it is protecting their information and that it has good data hygiene. And I think there is wide recognition across industry as well that it is also in our interest to ensure that we have some of that same basic governance in place when it comes to the development and use of AI. Thank you. Yeah, and NIST has been used in more modern times around cybersecurity frameworks to a great degree that governments as well as corporations need to adopt those standards or even to ISO standards which one of us may have heard from manufacturing world things of that nature. Other questions from the committee members? That is number four. Sorry, my wife is calling me at the same time. We have a homecoming parade. I hope the convertibles run and so on. Quick question because you said since 2005 you have been trying to get a privacy law on the federal level. Am I understanding then because what you all have in companies like you all is a... This is our generation's nuclear bomb, right? I mean as far as the power of changing the world and everything, you know, as far as nuclear energy and the land and split to add them and everything. And I just want to... So pretty much when it comes to deploying AI and how you use it then does it come down to the ethics committee and your corporation or whichever corporation is using it right now? Has the how it's being used across our society or are there some laws that affect how you can use it? Because I just can't imagine like when we all watched Oppenheimer here lately. You know, could you imagine if everybody in the world got a nuclear weapon and figured out how to split the atom and then we decided that we were going to come up with some kind of way to how it could be used? Do you know what I'm saying? I think so, Senator. So there are certainly some laws that would apply to different aspects of the use or deployment of AI. So to the extent that adjuristicion has a privacy or data protection law on the books, there are only 13 of them in the United States right now. In contrast, market contrast to most of the rest of the world by the way. Those laws would apply to the collection use of data. For instance, to the extent there are civil rights laws or insurance regulations and those sorts of things and if AI was to be used in order to make decisions that would impact those spaces then those laws would continue to apply. And we have sort of basic consumer protection statutes. There is the Federal Trade Commission Act Section 5 in similar UDAP or consumer protection laws that every state has which bars unfair deceptive trade practices. If AI is going to be used in order to commit an unfair deceptive trade practice to deceive consumers to provide fraudulent products and the like, those laws would still apply. Even if AI is going to be used to make deepfakes about someone in a way that would defame them, the sort of traditional tort of defamation would arguably still apply. But it is true that we don't have any sort of comprehensive AI law on the books in the United States today. And there are certain gaps and this is part of the conversation in the law that I think people are recognizing need to be filled in. Thank you. Other questions from the committee members? W3. Thank you, Chairman. My question is, it's kind of twofold. If you're growing cotton in South Georgia and you seeds and you want the result of that cotton plant to be a certain size as an endpoint or if you want that cotton plant to fail, if that's the created part from the beginning to the end, then AI can be utilized to create great production or complete failure. Would you agree with that? I think AI is a tool like lots of different tools and it could be used for bad. It could be used for good. Okay. So my concern is that when I look at a huge supporter of private enterprise, but each organization that has AI, when I see principles such as fairness, reliability, safety, inclusiveness, those pieces which are really important, but it's based on the definition of the company which is creating it, how do we create?\n\nprotection to protect end users so that we're not creating AI that allows a failed cotton plant. Does that make sense? That's kind of where I'm going. We need laws. I mean, it's why I think it is important that there be a robust conversation right now about what the benefits are, where the potential risks of harm, what laws currently apply, and where are the gaps? And where do policymakers need to fill in those gaps in a way that will continue to facilitate innovation while addressing risks of harm? I agree with what you're saying, what I'm saying. I think that's part of why we're all going through this process now to figure it out. When we make laws, that's usually to keep the good people from doing the good things. Right? The bad actors are going to go out there, right? And the question then becomes, is how do we then penalize punish, do whatever we need to to try and dissuade them from doing that in the future? But with this, we know there's going to be bad actors, whether they be on a business scale, local or a full state actors, hopefully working together as a team, with all the differentiated parties, we can observe that the mess we possibly can. But If I can say something, something that they both touch on in part of discussion is, you know, we're talking about 13 different privacy laws so far. We're not talking about building a law that protects a static structure within the state of Georgia. This is invisible, transported across state line information, everything. When we talk about each state developing their own deep fake policy and all this other stuff, this has to be almost on a federal level that it happened, but then my concern then, we're going to trust the federal government. And I'm going to use a terrible phrase here. So we're going to come up with a new world order that will basically lay out. I'm starting to have flashbacks of Philip Dick novels in my head where I can't walk anywhere without seeing the advertisements and billboards that they want Randy Robertson to see. And so, yeah, I think this is one of those mind fields and I can't thank you enough for this joint committee that we have here. But this is insanity and we're going to have to figure out how to grab this bowl fast. Well, and what you just brought up, Senator, is a great point. To some degree, that novel is already coming to fruition because you have location services on your phone. It knows everything you've got. You it's dropping your real-time ads of what's going on around you. I think it's important because Senator Goobin brought up data privacy. We talked about the 13 states that have it in place. Data privacy is different than AI, right? This is the protection of all things that you might hear the the terminology PI, personally identifiable data or information about yourself, right? So that's not having everything about Russ and his family and all those blueberries sites you logged into, then available for someone else to try and sell you strawberries, because I think you like everything berries, right? And we've seen that everywhere today. So we need to do that separate from all the work we need to do on AI. So this is pretty big. Go right ahead. I'll just say this and I'm like, like you said, this is the wheel. You said at the beginning of this meeting. Yes. It is, I've only been to these committee meetings and I'm just old south Georgia phone boy, but it is apparently to me, I mean, it's readily apparent that our laws are way behind the technology on this. That's that's the part that I think is that I am a theft or deformable. He's that blueberry millionaire. Question for Senator Page. That's my question I guess going back to this is if there's a bad actor, and that kind of goes back to what I was asking about how do you define certain words is because what is a good actor in the eyes of any laws we pass, well, there's always going to be a bad actor and that bad actor doesn't have to be somebody that's here. I mean, that's so how do you regulate laws by regulating laws to regulate that bad activity when you know, it's really going to come back. That's what the concern is. It really comes back to the actors. I mean, or trusting the actors to do the right thing. Well, and to some extent, your question, Senator, goes then to the question of enforcement. And if once you're able to get the laws right, laws that establish the right balance in order to require governance, require consumer protections, and all those things while enabling or facilitating innovation, how are they enforced? And that is a big and important question. And you know, that's a great point. Let's say we were to outlaw deep fakes in any campaigns. How do you enforce that? Right? Was it a was it the opponent that did it? Was it a third party group? Was it some 35 year old than his mom's basement? Right? We don't know. On the other side of the world. It's right. Thank you, Chairman Payne. Chairman Thomas. Thank you. I appreciate you being here today. You know, first of all, Senator Robertson, one of the comments that you made it. I'm a big believer in federalism. I believe that let's let the states, let's let ever, we got 50 laboratories, right? So sometimes when we have 13 different versions, I'm actually okay with that. I think one of them will shake out a little better than the other. But I, I, I, I, oh, are you good? Yeah, yeah. Yeah. I don't like when the, the big one comes down and tells us all what to do. But when I started looking into this and this is something we talked about a little bit on the House committee, when I looked at this, I said, okay, let's just forget what this technology is. Let's just forget that, right? We'll see if we can go back and look at press, the historical precedent of what happens every time we have any type of major revolution in technology. And, and it's happened before, okay? We've gone from, you know, horse and carriage to, you know, cars we've had, you know, industrial revolution. And it seems like there seems to one common thread that I've seen historically. And that's that individuals in that society get together create this, you know, a technical society that then, you know, sets the, the standards for how that, you know, that industry should operate, for example. We, you know, in code we, we cite, you know, international building code, right? We don't, we don't write the law. We just say, you know what? We accept international building code to be our standard of how things should be done. Because quite frankly, passing legislation is a much harder than changing code when somebody recognizes that there's an issue with the code. Is there, you know, and I know you talked about NIST and I'm familiar with them, but do you see any industry, the industry getting together to create a standard? Well, yes. I mean, the industry is participating in robust conversations, not just with policymakers, but amongst themselves about what standards should look like in this space. So for instance, and I agree with you that we certainly think it's important to have industries voice at the table because we're the ones who are making the stuff. And we arguably know as much as or more than anybody else about how it works. So for instance, Microsoft is part of something called the content provenance CP2A, the content provenance association, which is consists of a group of industry players who are attempting to standardize the method of alerting the public when they are consuming content that has been produced by AI. So providing a watermark, for instance, if you see a video or a photo that has been produced using synthetic AI, there are other conversations that are happening about the kinds of basic safety standards and protocols that companies are to adopt when they are developing AI, et cetera. So we think that is an important part of the conversation and to the extent lawmakers are considering what rules should look like in this space, they should have all of that in mind as well. I've got a follow up question. Could you mind, could you pull up that slide with the data stack on it? So one thing I noticed, you've got five, but really I think it seems like as you go from bottom to the top, the bottom is farther away from the, I guess the inconsumer with the top being the closest to the inconsumer, which is that the fair assessment. Yes. So typically when you're seeing policy making, is it safe to say we're focused more on the API services and applications and of things versus maybe we don't really, I'm looking for the lowest hanging fruit, I guess, is what I'm going after. And I don't see the bottom being anything that we really need to touch with policy at all. It seems like to me it's the top two. I think that's right, Senator. And most of the focus we've seen from policymakers thus far has been focused at the application layer. And how might applications utilizing AI be used? How might they be used in ways that could harm individuals, particularly if they're going to be used relating to important decisions in their lives? Yes. So if I was just going to kind of take a slice off the top, it sounds like to me applications would be really probably where we should focus in any time that we put in specifically, like I said, this life, safety, liberty aspect and how all those applications may affect, I guess, those particular parts of life. Would you say that that's kind of where you're looking for the most direction? That would be our suggestion, Senator. And I would consider included within that category, not just rules around what happens if and when people are harmed in some way by the use of technology, but also all of the governance issues that I mentioned, what kinds of governments, procedures, what kinds of personnel might say government agencies or other organizations want to have in place? Yes. Thank you. I just want to let you know I serve in the lower chamber. So have you're demoting me calling me a Senator? Thanks. It's aspirated. I thought he was Mr. Chairman. I got my Senator right here. So no, this was seasoned. So I'm just, we're just picking, this is just how it goes. It's all part of the fun. Yeah, and thank you. I think like said, the big thing to me is just trying to figure out what those guardrails are. I'd really rather see industry take the lead on it and us adopt what the industry sees at the standard, but I don't think we're there yet. So I appreciate it. Thanks. Well, I've even seen you well, the grounding standard. We talked about this at our last minute. They're coming up with AI frameworks, you know, for, you know, what universally allowed electricity to go across the country and the globe. So it's interesting to see who or many will be the foremost authority on that. It's interesting of what Chairman Thomas was talking about. I actually think that we might even get all the way down to the third wrong there, depending on how things evolve over time, because I believe that's where some of the concern may come from as well. Other questions from the committee members, before we close out, we have kept you here late tonight. That just almost feel like we're just telecommunications industry all over again. In the 40s and 50s is what I feel that we want something for Georgia, but it has to go across state lines and be able to provide the connectivity. And I think that's where a lot of the fear comes in. Pressure vessel design and boiler design. ASME was founded on the fact that boilers kept blowing up and kept killing people who get hurt and killed, which is, you know, the nefarious side of this technology, but it didn't necessarily take an act of Congress to get it done. What happened was the engineers got together and said, this is the standard, this is how you're going to, we are going to do it. And then we accepted that as our policy for the state. So I think that's the best path forward, but I agree. I know you and I usually agree on that. Yeah. So. Ryan, thank you for your excellent presentation. We really appreciate it. We will look forward to reviewing that in a little more detail. And without a doubt, following up and asking for your wisdom as we continue through this process to develop as Chairman Thomas said, the gold standard. Thank you, Mr. Chairman. So before we wrap up, just very quickly, I want to thank Ryan and Jake both thank all the committees for coming together. As we talked about today, this is massive impact. There's a lot more work needs to be done. The last thing that I will say is what we're leaving is, you think you mentioned the industrial revolution. Since the industrial revolution, almost every major leap forward that we have seen this country of innovation has somehow impacted blue collar jobs. Now, it may have caused them to go down in one area, but very much up another. This is the first time in human history. This is going to go after white collar jobs. And in massive percentages. In some industries, it could replace anywhere from 30, 40 or 75 percent of their workforce, which is something we have to think about for how does that impact our state and workforce in the future too. So thank you for your time. Thank you for being here so late. And we will work on the next scheduled meeting soon. This meeting is here by June.",
  "updated_at": "2026-02-14T02:01:13.274922+00:00"
}