{
  "video_id": "I_gkDOXkEOY",
  "url": "https://www.youtube.com/watch?v=I_gkDOXkEOY",
  "title": "11/1/2023 - Senate Committees on Public Safety and Science and Technology",
  "chamber": "senate",
  "session_type": "committee",
  "session_year": 2023,
  "day_number": null,
  "video_date": null,
  "duration_minutes": 173,
  "summary": "Here is a structured markdown summary of the key points from the Georgia 11/1/2023 - Senate Committees on Public Safety and Science and Technology session:\n\n**Overview**\nThe session focused on educating committee members on the impact of artificial intelligence (AI) and laying the groundwork for future policy discussions on the responsible use of AI in Georgia. The legislators acknowledged the transformative potential of AI while also highlighting the need for appropriate guardrails to protect Georgians.\n\n**Key Actions**\nNo specific bills were mentioned as being passed or referred during this session.\n\n**Bills Introduced (First Reading)**\nNo new bills were introduced during this session.\n\n**Notable Moments**\n- The committee chairman gave an overview of AI, describing it as a transformative technology on the scale of the invention of the wheel.\n- The chairman highlighted the potential benefits of AI, such as the ability to cure cancer, as well as the risks that need to be addressed.\n- The chairman stated that Georgia aims to be the number one place for AI in the country, both in terms of education and business.\n- The chairman outlined plans to develop an AI governance framework within the next 6 months to provide parameters for the responsible use of AI in the state.\n\n**Votes**\nNo recorded votes were mentioned in the transcript.\n\n**Attendance**\nThe transcript indicates that the meeting was being livestreamed and that the chairman of the Science and Technology committee, Senator Chuck Payne, was unable to attend due to a car accident but would try to watch the proceedings online.\n\n**Adjournment**\nThe transcript does not explicitly state when the session was adjourned or when the next session will be held. However, it mentions that the committee plans to hold an additional meeting in December during a special legislative session called by the Governor.",
  "transcript": "We're going to get started here in just a moment. Wow, that usually doesn't happen that fast. That's great. Well good morning and welcome. We're going to call this joint committee meeting for the public safety and science and technology committees in the Georgia State Senate to order. I want to thank everyone for coming and we're going to begin as we begin all of our meetings with a prayer and I'm going to put my friend Senator Rusk good on the spot to see if we'll give us a quick passing. Thank you Mr Chairman. Pray with me please. Dear Heavenly Father, we just thank you for today Lord. We thank you for loving us. We thank you for the gift of salvation. We thank you for the ability to be gathered here today and learn more about this issue that we're going to be discussing Lord. We pray to the people of this state, people of this country. We pray for the situation in Israel. We pray for the innocence on both sides of the Mayor Lord. We just pray. We live in a complicated world and it seems sometimes that things are coming apart. It seems Lord we just pray for our better angels to shine through. Lord as we go about this meeting we just ask that you give us wisdom and disarm it. Jesus name we pray. Amen. Amen. Thank you Senator. I want to begin by mentioning that Chairman and Senator Chuck Payne who leads our science and technology committee was an unfortunate car accident about an hour ago. He is okay. His car is not. So he is up off of exit 277 with the great work of our Georgia state police filling out an accident report and he's going to try and bring his bumper probably the back seat of his car home. But he is fine. He sends his regrets and he hopefully will get in time home and time to be able to watch us online. I do want to mention that there this meeting is being live streamed. So all those that are listening at home thank you to the great work from our Senate press office. That will be available as well as we capture these via video as well for those that couldn't make either of those. As we begin today it's important that we educate our committee members. Artificial intelligence is a pretty hot buzz word these days and being a technology person by background. Sometimes we put these big words out there and people don't really understand what they mean. But I want to talk through this a little bit because today the goal was really to begin the education process for committee members. We're going to hear for some tremendous industry experts. Also want to thank a deep personal friend and leader in my community who is helping us do this which is Ms. Becky Stone behind me. So we've got a really massive lot of talent into this room. So as we learn and move forward we'll talk about what some of those goals are going to look like and ultimately keep performance indicators. I've spent significant time researching this issue both here at home as well as some conferences. And I want to help to frame this conversation to start. Some people think that artificial intelligence is kind of a disruptor. And what is a disruptor? Well remember when Uber came out versus taxi cabs that was a disruptor. When the iPhone came out a touchscreen really changing putting that computer power in your hand. 3D printing. These were big disruptors. This is more than a disruptor folks. This is like the invention of the wheel. It's that big and it's going to impact every part of our lives and it's happening in real time. Now artificial intelligence general is not new though. It has been around since the 1950s. It is just now an exponential point of machine learning and growth that is changing the life that we're living today. This is going to bring tremendous innovation to the world and benefit. I was on a call a few weeks ago and one of the speakers brought up a scenario. And he said that if we took artificial intelligence what's called genitive AI and ran it on everything in the public. Every person we were treating for cancer and every group that was researching cancer. We cure cancer in a week. Now was he 100% right? I don't know but I do know he was not 100% wrong. Now that also comes with great risk and concerns as well. It is for good can also be used for evil and we need to be very cognizant as well. We are going to have a lot of meetings to talk about this coming in the next years. In fact we're going to already schedule one in December as Governor campus called us back into a special legislative session. We're going to have some downtime during that. We want to fill that in and make sure we're doing some good work. Obviously this impacts how state government will run. How our local cities, counties, school systems will run. Our university system. It's going to impact our families. It's going to impact all of our businesses. It's going to infect public safety. And really from a work perspective and this is what's very different. If you think about from the time from the industrial revolution forward most of the great innovations impacted the blue collar workforce. Now those jobs tend to shift as innovations happened. This will be very different where it's going to predominantly impact the white collar workforce. So that is a big change for what we're going to have to think about. And as we are moving forward on numerous workforce initiatives this state we do have a lot of job opening. So this may coincide very well in some cases. Now what can we do as a state of Georgia at this point? First, early this week the Biden administration issued an executive order that we are sending information out some is in your packet. Some will be followed up on email of how the federal government will tackle this issue. In some cases we want that to be the case as not to have a patchwork of 50 different states of ways to handle things. That being said there will be specific stuff that Georgia will definitely have to work on. We want to look at best practices. Some of the work that Becky and I have already worked on is looking at what are other states doing? What's happening around this to make sure that we're pulling from what they're learning ahead of time? I want to make no mistake we do not want a stifle innovation here. In fact the exact opposite. But we want to balance that with guardrails to protect Georgians. And also to tell you this is evolving literally sometimes on a daily basis. So what we're doing now is just trying to get our paths great as it continues to grow leaps and bounds. Now as all of us know when we like to brag Georgia has been the no more in place to do business for 10 years. Thank you to Governor Kemp, Governor Deal before him and the great work by the Georgia Senate and House of Representatives and all of our key business partners. To maintain that in the future we have to be the number one place for artificial intelligence. We are in the number one place for education and AI at every level. We want to be the number one place in our cities or counties and in the state government. Again, tying together the number one place to do business in the very near future we have to be the number one place for AI as well. And as all of you know I'm very passionate about all things public safety. Governance, the evolution from a 911 call to how we will dispatch police fire, EMS is going to radically change and for the better over time as we can get to people quicker and offer better services. But we have to maintain privacy and all the things that we hold most important. Some of the key performance indicators we are going to be looking to do is really looking at the first six months, one, two, and three years out. My goal for us in working with the Governor, other executive agencies and all of our counterparts is in six months to come up with an AI governance framework for responsible use and make sure that we have parameters set for the state and we can also assist the local governments that we also serve in state as well as city, county, and schools. In summary, this is the beginning of a learning and working process. I'm glad we're all here. We're all in this curve together. As we prepare to take action in the future, it's my hope that we can continue to work together and leverage all these wonderful industry experts. And with that, we're going to hear from three today. We're going to hear from Fred with CGI, Peter with Dentons and Maria with AWS and what them do a full introduction of themselves. These folks were very specifically picked out because they're going to bring a very unique perspective of what their wisdom and knowledge is, how it's impacting not just the company and firms that they work with, but how they are serving their clients up to and including governments. At the next meeting in December, we're going to hear from people like Microsoft and other folks next. Those will be the first two education meetings until we get to more policy driven things in January when we turn a full legislative session. So with that in mind, Fred, if you would come join us at the podium, we're excited to have you here today. And I'm going to let you do your full introduction and take us away. Thank you, John. Good morning, everyone. Good morning, Senator. Thank you for having me here today. I tend to be a little excited about the topic. So if you need to slow me down to ask me questions, please do. I think it's important part of the process. Quick introduction and as I'm learning to click here on the clicker, my name is Fred Miscaui. I'm from CGI. We're a Canadian firm based in Morayad, Canada. And I'm part of our global AI enablement team and I'm mostly focused on AI innovations, expert services, applied AI. So how do we apply these models in the real world? And a lot has happened since December of last year. And I'm going to talk about that a little bit more. And most importantly, I'm going to talk about responsible use of AI, which is very appropriate based on what we're seeing today and what's been released over the past two days. So the paradox for AI is the challenge that we're experiencing on a daily basis. It's using AI to unlock the value that's inherent in those models and the technology itself. And ensuring that the outcome itself is aligned with human expectations, human alignment, human agency. We want to make sure that we empower our members, our employees with the technology and making sure that we can also balance the cost that may come with it. So most organizations are strategically unprepared. This is what we've seen. And I have a lot of conversations on a daily basis with clients on that exact topic. So one massive set of discussions that we have is around AI governance and responsible use of AI within the context of the enterprise. But what we found is that there's a lot of unknown about the technology, how to apply it, how to move forward in a way that's responsible and ethical, how to make sure that the output of these models are aligned with our society expectations and to avoid biases that are inherent to the datasets that we already see in the internet today. So the age of AI is here. I've got a graph here just to give you an example a little bit of the history of the technology. It's nothing new. It started in the 50s and 1957 to be exact, the term was coined by or 56 term was coined. And 1957, the first neural net was born, the perceptron. The neural net is a way to try to emulate the human brain cells. It's a very simple way to represent it. But what we've seen over time and as we developed and invested in the technology is that that very simple representation of the human neuron can actually have some pretty massive impact to our society. So as we're growing, you see this person growth in the technology. We've been involved since the 90s. What's been different really is what's happened over the past year with Chad G.P.T. coming out in December or the very last day of November. And the reason why is because of this, the prior to that time, AI was really used mainly as pattern recognition. So if you look at the way that you're processing information, just like today, as it started talking, you heard a little bit of an accent coming from me and then you added a little more information of the company that I was from and you may have made the inference that I'm from Canada. I'm actually from France, so that's a type of hallucination that we see on occasion. But this type of pattern recognition is incredibly important to the technology. So as you think about it, any type of pattern recognition in any dataset, in any human information that we've generated over time, is being recognized by these models nowadays. Now what happened at the end of November 1st, the day of December of 2022, is a realization that these models are not capable of recreating those patterns and generate data, any type of pattern in digital form that we may need for purposes. And that's been a massive boom to our industry and has also accelerated some of the concerns that come with the technology itself. Another thing that we've seen over time is the through popular entertainment. We've seen a particular set of messages that have been put out about the technology. Terminator Tron, I would recommend watching X-Makina in case you have not. That's a movie that came out in 2014. But across that, you see another pattern, which is to paint the technology in a potentially negative way. Now there's a lot of power that comes with the technology, but we also are very cautious about the impact that it has and how do we make sure that the output of the models are aligned with our expectations. So as we, at CGI, have gone through the process over the past several decades, we've learned quite a bit about governance, about responsible use of the technology, and about making sure that the outcome is aligned with humans, with us. And you'll hear that theme repeated quite often, human in the loop. Now in our industry, it's with CGI, where 90,000 employees worldwide, or 10 billion revenue, and we're in a group of peers that have invested heavily in AI. And the four key elements where we're investing, I think, maybe of interest to you and to the state, the first one is Antoin offering expansion, so the type of services that we provide to the consumers of our services. The second is talent capacity and capability that's training, making sure that we augment the level of knowledge and understanding of the technology with our members, our employees. And we attract also a new talent. And we've seen a little bit of that through the White House executive order. The idea that talent is incredibly important to the growth of this technology, and most importantly, to make sure that the output, the outcome, the value generated by the technology is aligned with our expectations. Good to market strategies, and that's the type of nature of the relationships that we need to build, global alliances, you're going to hear from Amazon today, Microsoft in the future, and others. These communications, these relationships are incredibly important, because of the hyper-scalers, what we call the hyper-scalers, the very large providers of AI services, and the underlying power for these services are important to the technology and to the growth of that technology. And then operational, the delivery excellence. So we're reviewing internally the way that we produce value for our customers. From a financial perspective, CFO is impacted every organization within our company is impacted by the technology and will be impacted. So within the next, I'd say two to three years of the rate we're going, you have a full transformation of the way we do business, by leveraging the power of AI models. And again, that goes back to why our AI models so important, because they recognize patterns, and they're able to recreate patterns now digitally. And that means if you're a CFO or if you're in finance and you're building a budget for the following year, instead of spending two weeks building a budget, you can create a first draft of that budget in a matter of minutes, and then to work through refining the details and making sure that you're getting where you need to be. So, accelerating the expertise curve. This is something that we've noticed on the ground with companies, with our employees, and the other companies are our clients. It's the acceleration of the expertise curve. So when you look at expertise, that's the business that we're in, expertise. It's very important to understand how do we get to a level of expertise that a pretty good consultant might be able to be built for. And it starts with a formal informal education zone. So you get through formal education, college, post college, it could be a trade school. But you go through some form of formal education, and then you go into concrete experience, right? And I'm going to fast-forward through this because you guys all know this. But what we're seeing ultimately is this. So with the use of AI models, we're seeing an acceleration of that expertise curve. We're seeing individuals learning new languages in half the time that we used to see even last year. We're able to deploy systems in six weeks that used to take six months last year. So we're seeing that on the ground today with the companies that we work with. And another thing that it also does, it also makes the top of the field a little bit more competitive. We've got a lot more people now, access to information, that also can be experts in their field with putting the energy and the effort and leveraging the right tools. And that means competing with our peers in our industry, making sure that we put that extra bit of effort and leveraging the technology to that next level of maturity to make sure that we continue to be at the top of our field. Now an important research paper from 1984, some of you may already know this, but I think this is really important to bring in this morning as part of your learning journey in artificial intelligence. There was a study in 1984 that was done, so no technology. But what they were trying to figure out is how do we get students to the A level from a C level potentially? How do we accelerate learning process? And how do we improve the outcome? And what they found universally through their data and their trials is that the more one to one type of connection you have with a student, the more likely they're going to come out with A's and B's. And the curve that you're seeing here with the tutorial in 101, now you're seeing a bell curve that is squeezed with a lot more students in the A category or the B category. And we're seeing that with our consultants as well. So as we're applying the technology, we're accelerating the learning process, we're enabling them to understand it and to get the equivalent of one to one tutoring with an expert in the particular technology that they have to learn. So what does it mean for us? As we're translating that into sample actions, for the end to end offering, that's leveraging gerative AI in our products to enhance the experience, to provide consulting services as well to make sure that our clients understand the importance of AI governance and responsible use. We're going to hear a little bit later today about the importance of regulations and laws as well. And while we're promoting is an environment and ecosystem where both are important to the safety of human agency. We need both laws, rules and regulations as well as companies take ownership of the path forward with when leveraging AI. Establishing principles on proper use of gerative AI. It's not just when you should be using AI models, but when you should not. So when it comes to students, for example, I do that with my own kids, teaching them when should you use CHEDGPT and when should you not? I used CHEDGPT two days ago to ask for the top 10 questions for particular test and chemistry and the answer so that I could ask the questions to my son and I actually knew the answer. This is because sometimes you forget. So that can help parents to also provide that one to one to one to one for students. And we're seeing that with our employees as well. It provides us an acceleration. And the more we use the technology, the more we embrace it, the quicker it's moving. And that's another thing that maybe I'll leave you with as a concept for today is the idea that this is moving quickly. And because it's moving quickly, what we're doing is we're putting guard rails and guide lines in place so people understand the intent. And because we have learned very quickly that it's impossible to stop people from using the technology. One of the first questions that I ask our clients is do you know how many of your employees are using CHEDGPT today? And universally the answer is we don't know. And ultimately that could be an issue for some of our clients, especially in different fields like the medical field. So we need to make sure that we have proper governance in place, guard rails, as well as data gathering practices and reporting to understand and be able to answer that question. Moving on, those benefits and the value that we see are really geared around mainly large language models. Now we also have what's called diffusion models in computer vision. But a lot of the benefit that you're seeing, the value that you're going to be seeing for our own employees in the state of Georgia, is going to come from these five categories, generation reduction, transmutation or transformation, knowledge access and immersion behavior. So here are some examples of those. So generation if you need to generate a new text, a new clause in a contract, right? That's the type of generation that we're talking about. That can help accelerate the process of responding for us to RFPs. So in addition, the ability to take a large amount of information like the EU AI Act over 340 pages of and be able to reduce it in a way that enables you to get the gist of it before you start digging deeper into it yourself. So it really augments and helps you learn what's happening in the space today a lot quicker. So not only is the AI space moving quickly, but your ability to absorb the lessons from what we're doing today is also in your hands to learn quicker. Transmutation or transformation, an example of that would be, for example, if you have a legacy code base and you need to transform that code base into something more modern. Until today, a lot of times the cost associated with modernizing software algorithms is just too much for a lot of our clients, so it just doesn't happen. And now what we're seeing with the technology is lowering that bar. So if you have cobalt, if you have old code that's requiring a lot of talent to operate and maintain, all of a sudden you're not able to modernize in a way that's a lot quicker and a lot cheaper. Knowledge access, we all know the amount of knowledge that engines like Chagypetia have been trained with, but it's actually very important to the work that we do because of that latent information. We're able to do some pretty interesting things like generating reports or the ability to generate reports for police officers and accelerate the process for them to get them more time with our communities. And the next one, the immersion behavior, these are things that we're starting to see and leverage the models for that they were not necessarily trained for. Things like strategic thinking, the ability to parse a particular problem, the ability to provide guidance on how to approach a given problem. All right, so the three trends that we're seeing in generative AI today and in AI in general, number one is responsible use. A lot of companies want to be able to experiment, they want to be able to move forward a lot of our clients even in the public space are in that space as well, but they need guidance, they need to understand how to move forward in a way that's safe and safe for the citizens and the employees. Enterprise contact documentation, that's probably the number one use that we're seeing today or retrieval augmented generation. And that's the ability of the models to pull into all of your documents, PDFs, or documents, PowerPoints, and for you to be able to ask those questions. What was what happened last month on this particular topic? Can you give me a summary of this particular policy? And what this means as well in the public sector is the ability to train new individuals very quickly, the ability to retain talent and understanding a knowledge and expertise that may have been working in the government for 20 plus years, now retiring, now you have the ability to tap into some of that expertise in a way that's a little easier than what it was before. And scalable module cost effective architectures for greater adaptability. What this means really is what we're seeing is it's not one ring to rule them all. We keep talking about Chad Gbt, but there's cloth to their others. Amazon has bedrock in terms of environments to build these models and experiment with them. We've got Vertex AI from Google. What we're seeing is a democratization of AI compute, where you're going to have an entire multi-model ecosystem that is being built for checks and balances, where you're going to have specialized models that focus on security, specialized models that focus on policy and making sure that the models used in the space are doing the right things. So that diversification in models, you're going to see an acceleration of that. All right, so now the responsible use of AI framework. At a high level, while we're seeing with our clients, it's this necessity. We want to make sure that we retain and we become the best tour of our clients data and the rules and regulations that go around it and making sure that they don't get in trouble. So one very important aspect is here in this slide, the three concepts of human in the loop, human on the loop or human out of the loop. That came out of AI training, but what we did is we took that concept out at the strategic level. And what we're saying is in terms of ensuring human agency and to minimize negative impacts with the technology on our populations, we got to keep the idea of human in the loop. How are humans involved in the value generation process of AI models? And what we're doing is a lot of automation. So you may have heard in IT, we do a lot of automation, robotic process automation, processes that used to be very manual, may have taken two weeks, were able to automate them in a matter of days, hours. But the important through the process leveraging AI models is to make sure that humans remain on the loop. Aware of what's happening, aware of how it's been produced, the value has been produced, and be able to trace exactly the decision making process and be able to potentially replace the value that was produced because it's no longer aligned with human agency. So the last one, human out of the loop is something that was used in terms of automation when we're automating full processes end to end. There is no case that we've seen today where human out of the loop is desirable or necessary. Even if you send a probe out to the next solar system, there will still be telemetry being sent back to our solar system. It may take some time, but we'll be able to be and understand the value that that probe is bringing to us humanity. So that concept, we're applying to everything that we do at CGI and E-processes and development. And how am I doing on time, John? All right. So when we work on contracts, I ask myself these very simple questions. And I ask myself, do we have control? Are we able to pause the AI model? Are we able to stop it or undo what was done? Can we repeat it? Can we understand the decision making process for the AI model? And can we repeat it? Can we trace it? And in a lot of cases, we have regulations, financial industry, and others where we have to show how certain decisions were made and be able to trace it and understand where the data came from and be able to trace it all the way to the output. And most importantly, is it secure? Not just secure from the outside, as we know, security is a prime concern for all of us, but also from within. Can the model start calling APIs that it was not designed to do? It was not supposed to do. Do you have the right level of control internally to make sure that some of the more scary scenarios don't come about? So then that's where our AI service lifecycle comes into play. We're starting for a minute, I find the problem. And I'm not going to go into the details here, but what I'm going to say is this. The data flow is incredibly important. As Senator Albert mentioned earlier, when we're looking at the data and the technology, this is akin to the internal combustion engine. And we're seeing data as the digital black gold. So these models are consuming data, quality data. They need to be unbiased data to make sure that the output is where it needs to be. And you're going to see an acceleration of the need for high quality data to be fed into those engines. That's why we refer to it as digital black gold. But that's an important aspect of the value lifecycle and making sure that the output again is aligned with human agency. Key questions that we ask our clients is no risks. And that's applicable as well at the state level and all of the public agencies that we work with. Do you understand the intellectual property and copyright risk associated with them bringing the particular model to bear? Do you understand the business operations risks? So if you're going to be using it for building a budget, do you know the type of risk involved in generating a first draft of that budget using the technology? How well has it been tested? And who is it going to be impacting potentially? Cyber threats and security risks. What you're going to see is likely an acceleration of introduction of specialized AI models that are dedicated to security. You're likely going to have all of us here in this room of smartphones.\n\nI'm going to make an assumption here. But all of us will have a model running on our smartphones that will help us protect against certain types of calls. As an example, I got to call about a month ago, and just for fun, I actually do answer those calls, even though I know that they're spam. But I had one case where the individual I was talking to me was talking with an Indian accent. And as I said, hello, that in the middle of the sentence, the accents switched to a midwestern accent. That was in the middle of the conversation. And I said one or two words. I try not to have my voice recorded during these conversations. But it switched in the middle. The models are already used to attempt to influence you. We've already heard of examples where the models are used to impersonate and love the one, where they'll call you and say, hey, have been arrested. Can you send me money over here? That's already happening. So what we're seeing is an acceleration of introduction of models to help counter that. So now what you're going to have is an acceleration of an arms race. You're going to have the bad guys are going to be developing their own models. Outside of the confines of our legal requirements here in the US, they're going to be used to attack us. And we're going to have to find ways to defend ourselves. And the best way to do that is to use the same technology. Data protection, privacy and confidential theorists. Do we understand where the day is coming from? The bias inherent to the data. And as we've all talked about over the past several years, the importance to make sure that these decisions are not biased against any given group within our society. We take that very seriously. And we understand that these biases are inherent to human behavior. They're just there. And we also see biases within the enterprise that we work with in some cases that need to be reinforced because of the nature of the type of business that they have and the nature of the value that they produce. And then you've got the annual hear more today about legal and ethics risk and compliance. So here are some examples. Google's responsible use of AI principles. You've got the seven. You can go to the website, and you'll have a copy of the slides. And you can read them, and you can dig into them. I would recommend it. It's really interesting to read. And it also provides some guidance on what the path forward might be. Microsoft is doing the same. I'm sure you'll hear from them in December on that topic. They have a really good website where you can dig into further what this means and why this is important. And what we did is we summarized a lot of that into three core pillars. So trustworthiness, ethical and robustness, and just kind of bring them together into groupings. And that's just easier for us and our members and our employees to kind of remember them. But ethics is an important aspect of what we do based on the nature of the industries that we serve and is across the board. From medical, all the way to finance, and retail services, we serve pretty much every industry. So we have to make sure that we understand the ethical ramifications of leveraging the models in the ecosystem. Trustworthiness is pretty wide. It also includes security, making sure that we understand the impact and how the attack vector is might grow. So how will bad actors be able to get into the system? And then robustness, which is another important aspect of that, which I think you may need to remember, is the cost. So what we're seeing is even $20 per month per employee at 90,000 employees, that's a lot of costs that we did not anticipate a year ago. And that means that when you look at planning, when you look at investing, you've got a plan ahead. So a lot of our clients are not able to make the move with the technology right now because of their financial cycles. And that means that instead of introducing the value in six months, it might be 12 months. So we're seeing some of that cost is a major component of what we do today. A little bit more detail around the three pillars. And you'll be able to see that in the literature. But we want to make sure that we have a full end to an approach of the responsible use of AI in the space. And the holistic approach to the AI value lifecycle, so AI strategy, governance, and operations. Three key topics that come back with every conversation with we have with clients. In some cases, even if clients have been using and experimenting with AI models, they're still lacking the maturity in terms of governance to rem that up, to scale it up. And that's where these approaches become important. So for the state level, at the state level, having the right level of AI governance and strategy will enable to put in place the right guardrails to feed into the engine, to accelerate the process of leveraging these models. All right, future outlook. Three trends. Based on what we're seeing today, going back to large language models and being great pattern recognition engines. And now we're going to generate a little bit of pattern recognition here, but clarity and transparency of human and interactions. We see a lot of experimentation on how humans interact with AI models. It could be a chat interface. It could be an invisible interface that really kind of assumes what the human is looking to achieve and then provide guided questions. And hence, governments and monitoring, that's automation. So bringing automation in the government's process, making sure that our organizations that we work with have a clear understanding of what's happening within the organization. And decentralization, modernization of AI ecosystems. You're going to have edge computing become very important with models running on phones. We're running on laptops, running on internal data centers, as well as public cloud environments. All right, three suggestions for a path forward. And this is based on the conversations that we've had with other states as well, and commercial entities establishing your own states responsible use guidelines. Number two, keeping the concept of human and the loop at the center of the discussion. So as you're coming up with guidelines, rules, and regulations, how are humans served? What is the impact to human agency for that particular process? And how do we make sure that we don't stifle innovation? And we help accelerate the process of embracing the technology. Why? Because if we don't, like I said, a lot of these models are being developed overseas, and we need to be able to fight back against some of the threats that we're seeing today. And finally, enabling the acceleration of expertise curve. So for us, means the trainers that we have within the company means our employees, and for the state would mean students and educators. Very last slide. And I'm going to cover this with a concept. I talked about the HG rating, the human-generated rating, so 4.8 out of 5. The reason why I'm putting this here is because of the transparency component of what we do. It's important for all of us to understand and know when generative content is being consumed. It's important for us to understand if there's a particular text, legislation, or contract that we're reading, how much of that was generated by an AI model. That transparency is not only helpful for us in human agency, but it's also helpful when you're training models. Because synthetic data can have a negative impact on the accuracy of the output of models. So we want to know that the data sets that we're leveraging are clean. We want to know and understand where those data sets came from. Who generated those data sets to make sure that the biases that may have been there when the training occurred are no longer taken into account when we train the second and third and fourth model. Because that's what we're talking about. We're talking about models being trained to train other models. So having that transparency in generative rating is important to us, which is why I did here. Thank you, everyone. Thank you so much. I was a very comprehensive presentation. It was a lot for folks to absorb. I want to bring you back to some of the earlier conversation that you talked about. So from a timing perspective, one of the things that you mentioned was, in two years from now, developing that budget, which may have taken a team of people to do, many weeks or months to do, now be maybe done in a matter of minutes without all that input to begin that first round. Also, we talked about the return on investment for companies. Do they invest the dollars now? Because that could be an increase in cost for hardware, software, licensing, things of that nature. However, long term, that also is likely as people retire or we did workforce positions, they may not necessarily backfill in there. So can you expand a little more of how you see that kind of timing curve happening for investment versus return? So we're seeing investments today and it's done through experimentation. So the right AI governance layer, the right responsible use of AI is important to help the experimentation process and through the experimentation process, an iterative process. You're seeing what works, what doesn't work, what helps, what's aligned with our particular goals for the organization and what's not. So we're seeing that today and that's leading to realization that, oh my god, this could actually be incredibly helpful for this particular budget process, for this particular aspect of the process that we're involved with. So now we're seeing planning for deeper investments on the heels of that iterative experimentation. So that's a lot of what we're seeing. I anticipate based on what we're seeing, yeah, 12, 18 months, you're gonna see some heavy investments in the space, you're gonna see investments also in open source models for pure cost reasons and an evolution of a multi-model ecosystem. And didn't want to touch on education again as well. You brought up the model of how one to one ratio really can improve a student success factor. So typically in education is the largest part of our budget here in the state of Georgia and a traditional classroom of 20 kids. There are four kids that are excelling. There are four kids that are behind. There are 12 kids in the middle, right? And that's just typically the way it works, right? So what we're doing is we struggle sometimes to balance that because we wanna make sure that the kids who are a little behind, we get them up to speed or even above. The 12 in the middle are just there, they're caught in the middle. And then there's four that are too far ahead, right? In some cases we don't wanna slow them down. We might take them to another classroom and put them into an advanced program. What I'm seeing and I want your thoughts on this is now those four kids that may have been struggling right now with the views of AI, they can almost have a one-on-one experience with AI to get them caught up much quicker and to really propel education. Can you expound upon that a bit? Yeah, so when you look at that expertise curve, the four towards the bottom of the curve and I would also put a spotlight on the next four and the four after that. But what you're gonna see is an understanding from the students' perspective leveraging the technology of what they're capable of doing. So I think it's an acceleration of the realization of what they can and cannot do and help them compensate where they are having really difficulties learning. So you're gonna see that acceleration learning, the improvement in grades, but what will happen to the next four and the four after that will be the realization that going to the next layer of maturity, of capacity and understanding in the particular topic will open doors for them that didn't have open before. And now, and the four there at the top of that curve, as you mentioned, they may go to the next class or they may stay within the environment because they feel that they're connected to the students within the class. But now you've got to want to want tutor for that individual that can take the student to the next layer of capacity to an AP class level. And then because this is used across the classroom, you're gonna have data, deeper understanding of where the class is and the type of difficulties that they have. They might be a particular concept that the educator covered, but the way they was covered just clearly do not resonate with the students. And AI model can help compensate for that and realize that this is a problem across the class and help not only those bottom four students, but also the entire class. And that's why you see that kind of, that squeeze of the curve and the overall grades for the class to go up. This is very exciting back in 2015, I passed a digital classroom act. And the goal is technology can be the great equalizer for all kids in education, whether urban, suburban, their rural, socioeconomics, it really allows all those boats to rise in that tide of using that. I'm monopolizing the questions here and we turn it over to the committee members to see if they have some questions for Fred. See Senator Dugan? Thank you for being here. So Senator Albers is the technology expert, I can't even program a smart phone. So I'm coming at you from a different angle here. But on slide 29, and this is where it comes out, I am smart enough in technology to see the benefit of AI moving forward. I do get that. But on slide 29, you have three categories in there. My question, I wrote a whole bunch of stuff down, but my question is really, when you're talking about the enhanced governance and monitoring, you may use the term ethics multiple times in your presentation. Who's ethics? Would even in this room right here, where I have the highest regard of everybody sitting here, every one of our ethics is going to be different. And if we do ethics based off governance, who's governance? It's not a Georgia thing, it's not a US thing, it's not a North American thing, it's a world thing. Who's are we using? Because with all the potential to do good in the wrong hands, I do see potential to do bad. And there are cases throughout history, a recent one on a much smaller scale here that would have been in the pharmaceutical industry. They're going to introduce these drugs that are non-addicting until we find out that not. How do we get ahead of this to make it so that the potential for abuse is mitigated as much as we can AI possibly? It's a great question and it's a question that comes up in every conversation that we have, every public discussions that we've had that comes up. And what I'm seeing, Senator, is an acceleration of the conversation, whereas because this is now introduced in the ecosystem, these core discussions are on ethics. What is important to us within the confine of our group of our society? What is important to us? That accelerates the conversation. The conversation is out there, people are discussing, well, what is important to us? And why do we want to make sure that we apply in this particular context? And that discussion is a very democratic process. It's an important aspect of the process. And we're seeing that at the commercial level, as well as a public sector level, and you need to continue to have that conversation not just initially, but through the entire life cycle of use of these models. And eventually, I think we'll get it right. And if not, we'll be a lot closer to where we are today. Just one follow, if that's true. And I don't know this to be a fact. It's one of those things that's heard here say from my entire life. One of the differences between the settle and understanding your French, which you work for Canadian company, settling the Canadian West and the US West, why the US West was so volatile and the Canadian West was not. As kids in Canada, they sent law enforcement first. And then the population followed. And the US, they sent the population first. And then much later, law enforcement followed. What you ended up having was both of them were democratic type atmospheres. But the safety net was put in place first to prevent the violence. And I'm not saying that we're going to have shootouts in the AI saloons here. But that's where I'm getting that. Should we not have as much as we possibly could safety nets on the front side instead of, well, we'll get to it when we get to it? It's, I don't know that I can answer that question without getting in trouble, Senator. But what I can say is the discussion around that process is important. And the balance between sending law enforcement first versus sending the settlers first, having the right balance for a particular group of individuals involved in that, it will be important. What I do know, being an immigrant to this country, being a US citizen now, is that there are a lot of benefit and value to the approach of making sure that we don't stifle innovation early on. And that way, we can come up with new interesting innovative approaches like the internal combustion engine with Henry Ford. A lot of these new techniques, new inventions that came about were because we got in trouble, because we were moving quickly, because we had to react quickly. And so in a lot of ways, I think this is a great part, a healthy part of the discussion, a healthy part of the process. You have to bring it history. You have to learn from history. All I ask is that we make sure that we look at also the history of innovation and the value of the benefit that it has brought to all of our society. Even marginalized groups of society, I've also benefited from innovation. Thank you. Senator, I love your history and analogy. It's apropos. I think in this case, it's already going. I think we want to make sure we get alongside of it to go with it. That's kind of what I was asking. Question, Senator Jones. Number nine, thank you, Mr. Chair. So I have a question. In any organization, a lot of times when somebody leaves as they've been there 20 years or so, we say, men, that institutional knowledge is leaving. But yet, on two of the slides, it had employee training to access the institutional knowledge management and access to institutional knowledge, which usually means those intangibles that aren't in the book. Can you explain what you mean? How would you be accessing the institutional knowledge? It's very interesting there. Yeah, and might take longer than this particular committee and I'd love to have a discussion on that after a fact. But here's what we're seeing. When you're feeding the right datasets within these models, you start seeing patterns. And what we look at is, why are the impact of that particular individual's retiring on the organization? What have they produced in the past? What decisions? What emails have they created? What later correspondence have they done? When you ingest all of that, you get a pretty good feeling for the level and type of expertise that an individual has. And then working with the individual, knowing that they're going to be retiring over the next six months, you can augment the model, fine tune it, if you will, and make sure that you have a better view for what that expert was providing in terms of value to the organization. You can definitely do that as a way to help. But even in that process, you can get in trouble very quickly because now you're making inference, you're making assumptions based on that, which is why human and the loop is incredibly important. No matter what, somebody has to come in to kind of take over the role, or at least to have that level of responsibility for what that individual retiring had, and making sure that you have that human agency that's still represented in the value generation process. Other questions? Senator Goodman, number three. All right, thank you. Thank you for being here. Thank you, Mr. Chairman. Thank you for being here today. My question is, I guess a little more broad, but I certainly like Senator Dugan, I guess, I'm smart enough to see the value of this, but also I think pragmatic enough to see the downside as well. And I understand it's here, and we can't do anything with it. But I'm still thinking about those of us involved in public service. And I think about the last 20 years with SPAT, well, I was 1994. I was a junior in high school when I first got on the worldwide web, right? And from there, the internet's come, and now we've gotten social media. And I don't know if you've ever followed a fire in South Georgia, but when the fire, it creates its own weather, and it gets in the tops of the trees, and it's jumping three miles ahead. And if you're spotting three miles ahead, and if you're standing on a dirt road, it's put on some of its options, and it's like it's sandblasting your face. And I tell people, and we both do, with our constituencies deal with misinformation out there. And it's almost like trying to fight a forest fire in South Georgia, you can't get in front of it, to plow in front of it. All you can do is plow beside it. You know what I mean? And so when you think about the chaos in society, and what we're seeing today, and how it's really scary, you know, what we're seeing, imagine putting the tools of AI behind that, and what that's going to do to society. So that's what I question is, not only what it's going to do, as far as our society, and how we deal with others, and what the news that people see, and the things they hear, what it means geopolitically, you know, starting wars, what it means for some crazy person, because let's be honest, everybody in this world, and always stable, right? And somebody that wants to type in, if I wanted to poison the water system in the city of Atlanta, what if I wanted to create a dirty bomb? Those are a dirty nuclear bomb. You know, those are the kind of things that really concerned me, because if you go back in time and ask the scientists that develop a nuclear bomb, I bet you a lot of them would be like, I wish we'd left that alone, you know? So I just want to, like Senator Dugan was saying, you always have this tug between good and evil in this world. How do we stop this from being used for evil? That's the thing that concerns me, and it's almost like a little bit of the book revelations playing out in front of our eyes, and we get to talking about some of this stuff. I just, that's my biggest concern. How do we stop this from being used for evil? And I think that's a question that we're going to be wrestling with for decades, centuries moving forward. But what I do know, based on the history of human conflict, is that when you shine a light on the differences between people, oftentimes you can resolve conflicts in a diplomatic way. So what we're trying to do using the technology is to shine a light, make sure that everyone understands the information and it has the same level of information to be able to make the right decisions. That conflict is inherent to human societies. What we need to do is to make sure that we have the tools in place to be able to minimize the impact of the conflicts. And it's effectively what we've done in this country. The laws, rules, regulations, the system that we have in place, even the economic system in place, is there to minimize the impact of conflicts between people. And it's no different with AI models. When I talked about the importance of transparency and knowing what's generated by AI, what's not, we have an example of Tom Hanks, his likelihood being replicated, and then words coming out of his mouth that he never said. Something in the political space, we have a similar issue. What we need to do is we need to push for transparency and understanding of when these models are used and be able to also invest in technology that helps recognize when something has been generated. And if you do that, you'll be able to have a system of check and balances that will minimize the risk and the impact. The way I look at it, I tend to be an optimist. The way I look at it is that AI models are far like clear to solve the biggest problems that we've been wrestling with, with humanity, then they have to destroy us. So when we look at nuclear power, yes, is dangerous, but we found ways to mitigate it, to manage it. And now, thanks to nuclear power, we're able to go to the next solar system. We got to do the same thing with AI models. We got to put enough guardrails in place, put human in the loop, make sure we keep supporting human agency to make sure that we minimize the impact and move forward. So that's my take on it. But you're right, there are a lot of risks associated. And those risks, whether we like it or not, are there. It's out the genies out of the bottle. And what we're doing in the industry is to mitigate those risks, put in responsible use of AI, putting governance, putting models in place to validate that these policies are followed. And through technology, we're able to manage and mitigate the impact of technology. Zappo, I were having these meetings, Senator. Thank you. I'm going to go to one final question from Senator C. Thank you for all of what you share. My biggest concern, though, is having gone with seniors when we embrace and we're not embracing technology. I see this has been another hurdle. We're going to have to get through with that age population. As far as I know from the community centers that I do in my own district, one of the biggest concerns are seniors being taken advantage of. And that would be a scary thing for them. Well, us, because I don't remember the senior group. But speak to me about that, if you will, or if you have anything to actually add as an addition, because I literally had a study committee where we had to, well, I was able to get some tablets to start helping seniors embrace technology. And then when the phones came out, look, most of them all have cell phones now and are dealing with that. But that is a group that concerns me. And you're right. It is impactful for those of us in technology we have not been necessarily kind. And as we move forward, the user interfaces were not necessarily following the exact need of the populations that they were serving. Part of my background is in human-centered design. And that's why, one of the reasons why I put this here on this particular slide, the interaction between humans and AI. What I've seen on the ground right now is that the way that you interact with the technology is in much more human natural way. And therefore, it's actually easier to get value from the technology now through these chat interfaces than they ever were before. And what we're seeing is the technology evolve into a kind of operating system for interacting with computers. Whereas before we had clicks, right, you had to figure out, well, how do I get this parameter going? Which alienated a lot of the users. Now, you can actually ask the model, hey model, I would like to be able to book a cab to go to the restaurant to go see my grandchild. And it will help you through the process. And so what we're seeing is a next layer of abstraction above the technology that you've seen over the past 30 years to make things actually easier to interact with the computer. Which is why we're seeing that as more of a new AI operating system. One of the things that I want to mention while we're talking about this, and this is not just for the folks that you're working with in your district. It's for everybody here and anybody listening. Each one of your families needs a code word. And I mean this sincerely. And you do not email it to anybody, do not text it. But make sure they know it. Because for a long time, it started with senior citizens. They were preyed on, typically late at night, when they were sleeping, they get a phone call. This is your son, your daughter, your grandchild. Somebody's kidnapped me, or they're threatening my life. I need you to send me money right away. The difference now is it's going to be a FaceTime video. I think they can do this today. And it's going to look exactly like your family member. And it's going to sound exactly like your family member. And does somebody's going to have a gun in their head? And they're going to say, if you don't wire $10,000 right now, I'm going to kill them. If you don't have something to know that's not them, because it's going to come from their phone number, then you're going to have yourself and them in peril. So please take this as a good opportunity to work with your family. Come up with a code word that everybody can remember. But nobody shares outside of verbally to protect you and your family, and especially the folks in our senior community. Fred, thank you so much for this incredible work. We're going to obviously make sure everybody's got a copy of your presentation, but digitally, as well as in paper copy. Up, well, we've got one more question or not. OK, one more question. Senator Jones. Going back to that. Institutions of Knowledge, question I was asking. So OK, so when I used to be a solicitor, I was dealing with prosecution. And if I kept my case count down, what you're saying is you would interview me, find out how I kept my case count down, but you could also like on my computer how I prioritize my cases. So my question then becomes, you wouldn't necessarily have to bring me back in as a consultant to talk about how to prioritize cases. So I give you all this data, which you're going to be extrapolate out and extrapolate out and extrapolate out and know how I would do it to keep new solicitors keep their cases down. Am I going to be paid for that information? I'm just curious. And I'm assuming you're referring to the potential, impact on jobs within our society and the particular roles that may be displaced. We are already seeing roles being displaced, to some extent. What we're also seeing is a new generation of roles that are being created in order to deal with it. So in the case that you mentioned the prioritization of cases and the consulting services that might be needed to make sure that you augment that process, while we're seeing is in a case like that, what I would recommend is to make sure that the model that's providing that type of guidance is also very transparent so that you know how this came about. And for the consultant that was working specifically on guiding and providing feedback in that space to make sure that they work within the industry, leveraging AI models on how to get to the next layer of maturity. Just like we're seeing with AI models, which is this new stack layer above the old technology that's now kind of acting as an operating system to translate it, you're going to see this next layer being developed with people that use AI models. Where you got to need people.\n\nare able to converse and be able to advise and consult on how do I best use the technology to get to this next layer. And that's what we're seeing on the ground today. So yes, it's a question that's in the news and it's right, there will definitely be impacts. It's not quite known at this point, the level and the type of impact. I think there's things that we can do to mitigate the impact. But if there's one thing that I've seen this year based on working on this day and day out is that it's a very evolving field that is generating different needs for new types of skills and new types of people. So for me, and I say that to our employees quite a bit, it's, and you probably have heard this, it's not an AI model that's going to take over your job, it's going to be someone sitting next to you that he's using an AI model that's able to produce something you're doing 40% faster. And we're going to see more and more of that. And that's why all of us will be using AI models morning to evening to help a guide to help protect and to help strategize in all of cases. Again, Fred, thank you so much for this great presentation. Again, we'll be sending your information out as well as contacts so we can follow up. And there's a lot of work ahead. We appreciate it. Thank you, everyone. Appreciate it. All right. Before we bring up Peter next, I just want to bring up a couple of points for people. If you don't have, these are free tools. You should probably start looking at everybody talks about chat GPT. That's a big one. Claude is another one, Bard is another one that are out there. These are good tools to start playing with. And I'm going to give you a couple examples of how I use them. A lot of us here write for like a local magazine or newspaper, right? Well I write for two. And one of those I put in there about six months ago, I need an 800 word article from a Southern state government elected official on effects of a pre recession with increasing interest rates and inflation and a couple other factors. And in ten seconds, it wrote me an exact 800 word article and I've been changed one word. It was that good. I referenced that I got this data from artificial intelligence, but in Fred's model that he showed just a moment ago on that scale at the end, it was zero. Right? But let me give you a couple other examples. I also had a group come meet with me that wanted to change my opinion on the death penalty. They came in met with me and after they're done, I went into all three systems and I said, give me the pros and cons and facts on the death penalty and your opinion. Well, chat GPT said, well, I'm a computerized model. I can't give my opinion, but here are all the facts and figures and pros and cons is outlined. Well, I went to the next one. Clawed gave me the almost the same exact type of an answer. Bard told me while death penalty was awful and terrible and why we should get rid of it. That's interesting, right? Another one I went in and I said, wow, I'm going to check myself. What does it pull up? When I pulled myself up, it said that I was a form away of state representative and I had all these different business interests and I went to college these places. None of that was true. It didn't even have my current information correct over where I was. So I said, well, where did you get your information from? I said, well, I pulled it from multiple sources. I said, well, you're wrong. They said, well, tell me. So I educated it. Well, now it took me as gospel, right? Which means anybody could have done that to anybody else as well. This is gets back to, I think, what Fred's talking about, kind of the scary parts of this. I don't know if some of you saw, not too long ago, one of them wrote their own Bible verse. Right? I'm pretty sure that the folks that wrote the books, the Bible, a couple thousand years ago didn't expect anybody to arbitrarily add one later on. But these are things that, again, they're not meant to frighten people, but they are meant to have us have a very healthy understanding of what the negatives and the positives are in each one of these. And I commend Fred for your words of the optimistic approach. I do think the greater good will be served from what artificial intelligence can do. So the reason why we're meeting in Senator Goodman, you brought this up, was to start putting those parameters and protections in place now and further to what Senator Dugan said is ethics. I mean, how often does somebody say that's my truth? Right? What is truth? So these are things that we're going to grasp with. And that's why we're going through this process. And I think it really leads us. And this was a little bit of an entry and a tee up for you, Peter, to talk to us about what's happening in one of the world's largest law firms. And I also want to thank you who flew here from California while your wife is seven months pregnant. Is that not right? So, all right, well, well, make sure we don't get you in the doghouse. So please take it away. I appreciate it. Thank you, Senator. Fellow senators, good morning. It's a real privilege to be here to talk about such an important topic for the state of Georgia artificial intelligence. My name is Peter Stockberger. I'm a partner with Dentons of Senator Albers. I know that Dentons is the world's largest law firm. I'm based in our San Diego California office where I lead our U.S. artificial intelligence practice and co-lead our global autonomous vehicles practice. So my practice day to day is working with both private and public sector organizations on navigating technology, the law, and the emergence of new technologies like artificial intelligence. So I'm really going to talk about four things today. I'm going to lay out the promise and peril of AI. And I put this slide deck together before the White House's executive order on AI that was released yesterday. And in their opening line, they use the phrase promise and peril. So I feel proud of myself. I'm going to walk you through how AI is being regulated today, both globally and at the federal level and at the state and local level, to give you a sense of what's happening today from a legislative and a regulatory perspective. I'm going to talk a little bit about what Fred talked about, which is this concept of being responsible in the use of AI today. And Senator Albers, you noted in your introductory remarks, your goal in the next six months is to develop a responsibility framework for the state of Georgia. A lot of other states are doing that. I'll give you an example of Washington in California. But then I'm going to really focus on Senator Goodman and Senator Duggan, you asked a question about how do we protect this in the future? How do we protect against the bad AI? How do we ensure that it's used for good? And I'm going to talk about a proposal that we've put forward in a white paper about a standards-based approach to AI. So I'm going to talk a little bit about AI today, but really more importantly, how you regulate AI tomorrow. So what is the promise and peril of AI? Well, I'm going to turn it. So I always like to just begin with basic definitions, like I'm talking to a jury. And what is AI? In its base form, AI is a machine-based system, a computer program that can make predictions, recommendations, make decisions on its own, that augments human intelligence. Now the future of AI is that it may replace human intelligence or actually exceed human intelligence. But that's the baseline definition of AI. So what is machine learning? Machine learning is a subset of AI. When you hear the phrase machine learning, it's a subset that is looking for relationships and patterns. So it's a deeper form of that computer process. And then deep learning is the next level of machine learning, which is a set of neural networks. Fred mentioned it's replacing the human brain, layered AI, to give you deeper analysis, deeper assessments on data. Generative AI is what we're all talking about. And this is a new form of AI as Fred noted, came on the scene in 2022. And this is a form of AI that can generate text, audio, center, alvers your example right now of the phone call and the kidnapping. It's generating video and audio. Fred you mentioned the telephone call from India that switched voices. That's generating a new voice. So it's this new form of AI that has remarkable capabilities that virtually imitate human activity. And the reason why we're seeing this boom right now is because there are a number of commercial tools that are just rolling out onto the marketplace. Chat bots are the ones that we all use in center, you noted using chatch ebti, use chatch ebt all the time with my five year old son to come up with funny bedtime stories. And these chat bots are powerful and they continue to get more powerful. There's another form of tools that are publicly available now and they're really enterprise level tools. And they're produced by the same types of companies. We have IBM, I know Amazon is here today. We have Microsoft. These are tools that are being given to companies and public sector organizations that sit on top of your data. So Senator Jones you were asking about institutional knowledge. How do you process institutional knowledge? That's what these enterprise tools do. They actually sit on your own data as a company for example. You feed it in and it's providing you insights and analysis that you never had before on your data. Microsoft for example is releasing what's called co-pilot in November publicly available. And I'll give you an example. Let's say we're on a Microsoft Teams meeting and we're talking about well I have some follow-ups. Let's create a slide deck for the next meeting. This tool once you close that meeting can generate a transcript, can generate the slide deck, can generate a follow-up email in your voice. So very powerful enterprise tools, very powerful commercial products that are on the scene today. But why is there so much attention on these tools? What is the underlying issue? Well as already been mentioned AI is not new, it's been around for decades. The rate of acceleration is new. And an analogy that I like to give is if you start at the top of the hill with a snowball and you roll it down and you say well the snowball can only get 30 times larger. Well it has a cap, it'll stop. AI is actually the acceleration of AI is accelerating and it's limitless because it continues to learn and it continues to improve. So that same snowball with infinite acceleration in a very short amount of time is the size of Mars. And that's what AI is doing. And right now we're talking about chat bots and enterprise tools. But the future is going to be an ecosystem of AI that connects the physical, the digital, the human. And that's the challenge. A recent analysis by McKinsey estimated that AI could add between 17 to 26 million US, trillion rather annually to the economy. So you can see where the promise is, extraordinary promise across every sector, across every industry. There is significant risk. And the risk is something I think we all know instinctually. There's privacy risk. There's loss of employment. There's disruption in industry. There's IP risk. There's copyright risk. We already see a number of lawsuits right now, pending in courts. And some of these larger companies by authors, the author of Game of Thrones sued these makers, saying that when you run a search in the chat and you say, write me a story about medieval earth, you're pulling from the Game of Thrones script that was copyrighted. And therefore it's a violation of federal copyright law. Those cases are percolating. There are other cases, for example, against workday, which is the large HR system. And that's pending in federal court and in the Northern District of California alleging that their job applicant search tool that employers use was trained on discriminatory data. Therefore, it's a violation of Title VII of the 1967 Civil Rights Act. Those are some of the legal risks that we see. But we're starting to see the promise in peril really sort of coalesce is in industry, in certain sectors. We've already mentioned education. Education is a critical point. For example, in New York, New York, the New York public school system took the position early on that they were not going to use AI. And then they realized that students were already using it. Teachers were already using it. So they changed course, and they developed a responsibility framework, and they developed guidelines for teachers. There's actually a new story just a couple months ago about public schools in Georgia using AI to teach students about different options and different topics. So education is a real accelerating industry using AI. Employment is another one. These tools are remarkable in scanning thousands of resumes online to search for the ideal candidate for your job posting. You don't even need people to apply for a job. It goes out and finds the right candidate. And these people will often, when they apply for a job, interview with a video system that analyzes facial gestures. Oh, this person looked off to the left too much. They may not be trustworthy. So there's a real disruption in the employment space. And the EEOC, for example, has come out with some very strict guidance on mitigating bias under the ADA in Title VII when using these tools and employment. Healthcare is another area. I work with a lot of large healthcare organizations. And the promise of using AI in healthcare, Senator Albers, you noted about the conversation you had on the potential to cure cancer. But it's even more basic than that. It's in the clinical space. They are testing AI right now and just responding to patient questions. Doctors are always flooded. You probably all have through your healthcare system some ability to send a message to your doctor now online through this email inbox. And doctors are inundated with these messages. They're using AI to respond to those messages. So you think you're getting a response from your doctor, but it's the AI tool that was trained on your medical records. So the promise in the clinical setting is there. The promise in the operational efficiencies are there. We're seeing insurance in setting rates, which is highly controversial. And we're seeing state insurance commissioners really dive deep on this to set guardrails around using AI in insurance. Construction is using AI to actually build out blueprints and build out plans. And that's a remarkable tool that's speeding up the schedule for a lot of construction. So you can see, and obviously in the public sector, we're seeing states like Utah, Utah, use it for cybersecurity purposes and for public emergency response. And so Senator Goodman, you mentioned about the wildfire example in Southern Georgia. There are a number of states now using AI to predict where the next wildfire is going to occur by looking at smoke on the horizon and analyzing it using AI. So there's tremendous opportunity on the public sector side, preventing fraud and unemployment claims is a big use case right now amongst states. And local jurisdictions, New York got in trouble for that because the system they were using, the AI system to prevent fraud and unemployment claims was blocking people out from actually submitting a claim because the security protocols were too high. So there's a balance, but the promise of AI in each of these sectors is significant. So getting to, we understand the benefits, we understand the potential downfalls, how is it actually being regulated? What's happening today? I think what we're seeing right now is a global struggle to understand how we should regulate AI. The G7 came out in May of this year and said there needs to be an international standard developed around AI. The G20 recently released a statement advocating for the same. And what we're seeing at the global level is we're really seeing regulation break down into three buckets. The first bucket is a risk-based approach. This is what the EU is doing. They're essentially with the EU AI Act, which is this significant piece of regulation that's being developed in the European Union. It's currently in draft form. It's being negotiated at the European Union level, but it would be a regulation that would apply to all EU member countries. And what it says in the current draft form is that we're going to rank AI based on risk. We're going to say certain AI is just too high risk, where it runs the risk of manipulation or it runs the risk of creating a physical harm. We're just going to ban it. It's going to be prohibited in the European Union. The next level, if it involves health care or employment or something along those lines, it's going to be high risk and subject to very strict governance protocols. So they're taking a risk-based approach, which is arguably advocates of it like it because it's clear people who oppose it are saying that it's too prescriptive and it's too looking at the now. And it's not going to contemplate the future of AI. For example, the first draft they came out with of the EU AI Act didn't contemplate generative AI. So they had to go back and rewrite it and renegotiate, showing you how this law is being developed. Right now it's in draft form. It's being negotiated. We're anticipating a final draft at the end of this year to potentially be passed by next year. If it is passed, it would be the first global AI regulation that applies beyond just a simple nation state. That's the risk-based approach. Canada is adopting that approach. Brazil is adopting that approach. The US is contemplating that approach in some of the US Senate hearings that Senator Blumenthal is running. There have been several senators advocating for a risk-based approach. So that's one bucket. The next bucket is more of a market-driven approach. And that's the approach taken right now by the United Kingdom. This week, actually tomorrow, they're hosting the UK AI Safety Summit, which is being seen as a remarkable event in the AI community. There's 100 leaders of AI being invited by the Prime Minister to discuss the various issues that you all are asking about today. How do we prevent the future harm of AI? And the UK's approach, they've released a white paper where they say we're going to take more of a market-driven approach. We're going to be a lighter touch. We're going to let different industry work with regulators to find the right fit. We're not going to take a one-size-fits-all risk-bucket approach like the EU. Now there's probably a lot of politics going on. UK wanting to be seen as more of an innovation hub than the EU, but that's the UK's approach. And there are advocates at the US federal level for that approach. So that's where some of the tension is. The final approach is central management, and that's China. China has come out with very robust and draconian regulations around AI and generative AI, not surprisingly. And so that's the third bucket. We don't really see many other jurisdictions talking about that approach, but those are the three buckets that we see globally. Now in the US, we've got really a moving target happening right now. At the federal level, we do not have a federal AI law yet. That is currently being worked on. Senator Schumer announced this year that they are holding AI listening forms very much like this meeting here to understand how federal legislation should be developed around AI. And Senator Schumer publicly stated, we cannot draft legislation around AI like we normally draft other legislation. Technology is moving too fast, so we need a different way of approaching it. It remains to be seen whether they'll find a different approach to drafting legislation. For right now, we see Senate hearings, House hearings, and these listening sessions. There are several bills that have been introduced. Senator Blumenthal and Senator Hawley, for example, have introduced the idea of a licensing agency at the federal level, similar to the FAA, saying if you're going to launch or develop a high-risk AI system, you need a federal license to do so. So there are multiple proposals being bandied about. The White House has really been leading the charge on this. In last year, they released a blueprint for AI rights, which really started the conversation around federal approach to AI, the White House directed federal agencies to enforce current legal frameworks on the use of AI. So that's why we saw the EEOC, the FTC, the CFPB, all of our favorite alphabet soup agencies came out and said, everyone in the ecosystem, we will enforce our current laws on your use of AI. So the FTC, for example, said, don't over-hype AI. We will find that to be an unfair business practice. If you go out and promise AI, I can do something and you can't do it. It doesn't matter that it's a new technology, we're going to hold you accountable. The DOJ has said the same thing around Title VII. Interestingly, at the federal level, we've seen an AI risk management framework released by the National Institute of Standards and Technology. This is a standard-setting group within the U.S. Department of Commerce that is charged with developing technical standards for the government for federal agencies. And they most famously back in 2014 created a NIST cybersecurity framework, which is often used by public sector at the state level, local level, and private to benchmark cybersecurity resiliency. So you use this NIST framework, measure yourself against it, and we actually see written in the law, states will say, baseline reasonableness for cybersecurity is alignment with the NIST framework. The NIST is often used as a benchmark. In January of this year, NIST released an AI risk management framework where they say, if you are going to be using AI, you should measure your risk against this framework. So there are some states right now considering bills that would say at the baseline, if you're going to use AI in our state, you need to benchmark yourself to the NIST framework as a reasonableness standard. So that's active right now out of the Department of Commerce. At the state and local level, we see a lot more activity. And states are really taking a number of approaches. Since August, governors in a number of states have announced executive orders around exploring AI in the state. California is an example. Governor Newsom recently issued an executive order outlining a plan to conduct risk assessments to get studies conducted on the benefits and the peril of AI for use in the state. We also see governments at the local level implement responsibility frameworks. So Senator Abelers, you mentioned this six month outlook for this group to develop the AI governance framework. Cities are doing that. Washington state has done that. They're also developing procurement guidelines. So if you are going to be using a vendor as the state who utilizes AI, what are the contract provisions that have to go in there to ensure responsibility use? Washington has a procurement guideline that they've developed. They're also forming states are forming task forces and working groups, not unlike this joint committee meeting to explore the use of AI in the state. And we also have bills proposed to address high risk industries. So employment, healthcare, financial services, specific laws to say, you can or you cannot use AI in this particular space until we get our arms around the issue. So this concept, I'm going to talk a little bit about what this concept of responsible AI is, which is what all the states are starting to coalesce around and the federal government. But then I'm going to actually propose this concept of responsible AI is a band aid. It only scratches the surface and it actually will not address the future trajectory of AI. So what is responsible AI? The White House, the OECD, which is the international organization, the EU, NIST, they all lay out these principles. And they say, anytime you develop AI or use AI, you should do so in alignment with certain principles. And Fred mentioned Microsoft has their own AI responsibility framework. Google has theirs. Dentons, we've created a responsible AI framework to help clients navigate this space. But really, I break it down into four principles. One is AI should be valid, it should be reliable, it should be robust. So Senator Albers, you mentioned about using Chatchy BT and it said, things that were incorrect, has a validity problem. So there's a famous story of a New York lawyer who used Chatchy BT to file a brief with a New York court where he submitted fake cases that don't exist because he used Chatchy BT and said, find me five cases that discuss this issue and come up with this rule. And it spat out case citations, pin sites, very much looked like real case citations. And the court came back and said, we searched and we can't find these. And he had to file a declaration saying, sorry, I used Chatchy BT, it looks like they were made up. That's a validity problem. So if you're the law firm using Chatchy BT from a risk responsibility framework, you may say, we can't use this tool because it has a validity problem. Therefore, under this responsibility framework, it's a no. That's the concept of this responsibility framework. It gives you a prism to view whether or not the AI can be used in your environment. Safety and security are obviously key things. Transparency and explainability. This is the hard part. Do you know how the AI came up with its decision? And the current generative tools that are on the market right now are referred to basically as black box models. Even the makers of these tools can't tell you how it came up with the result. And that's because they're operating very much like neurons in our brain. We know that neurons tell us. They tell me to move my hand like this, but we actually don't know how it happens. And that's very much what's happening with a lot of these models. And so transparency is a problem. The EUA Act, for example, says that citizens of Europe will have the right to demand that an AI developer disclose how the model was developed and how it came up with the decision it came up with. And several of the large model developers in the world said, if that's the final text in the law, we just can't operate in Europe because we can't satisfy that requirement. So this concept of transparency and explainability is the wall that everybody's running into right now in using these tools. And if you have 100%, you say I have to have 100% explainability or transparency in how this actually came up with the result it did, you have a very hard time justifying the use of the tool. And so there's a trade-off. So I often see examples, for example, people using AI in the marketing department to generate email subject lines, pretty low risk. You're not using anybody's personal data, you're using it for a creative purpose. But if you're using AI in the hospital setting to draw medical diagnoses, to give somebody a diagnosis, that transparency and explainability needs to be 100%. Otherwise the risk could be too great of a misdiagnosis. And we're actually seeing insurance companies now tell hospital systems, if you use AI to come up with diagnoses, we're not going to cover you for a medical malpractice claim. Because the transparency and explainability are not there. Finally, privacy and fairness, this is a key component. Fred mentioned you have to be careful about bias data. All data is biased. So if you say, for example, as a state, we will not use any data. We will not use any AI that is biased. Then you'll never use any AI. So really you have to think about what is the bias you're trying to protect against? Is it unlawful discrimination? Is it protecting a certain community from a certain outcome? So this concept of fairness and bias is very complicated. It obviously has a lot of sociological roots to it and requires deep thought, I think, before you adopt a responsibility principle. So to give you an example of what states are doing in this space, Washington, for example, published interim guidelines for use's responsive uses of generative AI and state government. They also develop procurement guidelines that rely on that NIST AI risk management framework. So this is an example of a state actually publishing guidelines, getting out some meat to the state agencies to say what they can do around the use of AI. California is another example. We have this executive order signed just last month. Requires a report to be prepared for the governor about responsible uses of AI in the state calls on several agencies to develop their own use cases and plans and asks agencies to engage in AI responsibly. So I said before this concept of responsibility, it helps you in the immediate. Senator Albers, you asked what do we do today? Tomorrow, six months, two years from now. Responsibility frameworks allow you today to use the tools that are on the market today in a way that gives us some comfort that we've got human in the loop. We're protecting against some harms. But what about the AI of tomorrow? Because these generative tools, these companies are already on to the next form of AI. And that's what I want to spend the rest of the time talking on. The future of AI regulation. How do you actually regulate AI as a state entity when the technology is accelerating the way it is? So we know the call to regulate AI is clear. Only once a solution, we do not know the right path forward. That's what everyone is struggling with. That's what the UK is debating this week. That's what the federal government is debating. And the challenge, the reason why this is a difficult challenge is because AI is going to move through these three phases. We are in phase one, which is referred to as artificial narrow intelligence. These are a set of very specific computer programs that perform a very specific task. They're narrow in their application. They may seem remarkable and revolutionary, but they're narrow in what they can currently do. What everyone is working on is the next level, artificial general intelligence. And this is where AI can perform any task a human being can perform. It's the future of inter-\n\nActing with the operating system, and you think you're interacting with a human being, and the general intelligence that's connecting all of it is actually mimicking human intelligence. We're not there yet, but we're accelerating to artificial general intelligence very quickly. Artificial superintelligence is what Terminator 2 is about, the singularity, the final act of where AI is smarter than human beings can actually rewrite its own code to perform its own tasks in the most efficient way. This is often the fear about a nuclear attack, is that AI figures out that the most efficient path forward is to move forward without human beings. What's the most efficient way to get rid of human beings? Do X, Y, and Z execute this code? That's artificial superintelligence. We're not there either. But that's what everyone's talking about right now, the fear of getting there too quickly without the guardrails, without the protections. And really, I don't think AI is going to end up being this singularity. It's not going to be a single entity that controls everything. It's actually going to be an ecosystem of connected AI, smaller AI's, throughout the entire environment, physical, digital. I often use the examples we do a lot of work with autonomous vehicles of the intersection. Right now in California, fully driverless robotaxies are operating throughout the state. Nobody's behind the wheel. It picks you up like a noober. Takes you where you need to go. Those are autonomous, fully autonomous vehicles operating on the roadway. But what happens when the street light is an autonomous AI? The human being is crossing the street in a wheelchair that is an AI-connected device. The fire department truck, the police officer, everything is connected. Physical, digital, buildings, environment, the wind. That's the ecosystem that AI is moving toward. And that's where we're on a trajectory from automatic to autonomous, I always mispronounce this word, autonomous systems. So automatic is we walk through the airport and the sliding door opens and closes. That's an automatic system. That's a program that's responding to a task. There's somebody present, I'm going to open the door, I'm going to close it. Automated is what we saw with the revolution in manufacturing and robotics and replacing a lot of human activity. We're now in an autonomous stage right now where we have vehicles operating without human beings behind the wheel. But we're moving more towards this final stage which is fully interconnected, fully mimicking human beings. And really what you'll see this referred to as cyber-physical systems. These are internet connected, AI connected systems. And the diagram here just shows you all the different ways that these can be connected throughout our communities. So that raises the critical question. How do you govern systems that are actually on a path to becoming self-governing? The word autonomous means self-regulating, self-governing. AI will actually exceed human oversight. Well the challenge is that we can't govern AI the same way that we govern humans. And so our traditional way of law making, our traditional way of developing regulation does not actually fit this mold. And that's because machines don't respond to punishment or the traditional incentives of human beings. And they're also not bound by our own empathy and our own ethical concerns. Senator Duggan, you asked who's ethics? That's a challenge because it's whoever's building the model. So what we need for the future of AI regulation is we need a means, a method of actually encoding principles and guidelines directly into the AI itself. Not rely on the makers of AI to be responsible, but actually encode it into the technology itself. And that's where we come up with the call for standards. The G7 in May of this year said the solution to this problem is a set of international technical standards that govern AI. NIST, the National Institute for Standards Technology, has called for the development of technical standards. The G20 recently called for the development of technical standards. The UN General Assembly called for the development of technical standards. And then when everybody's pressed on what are those technical standards, everyone says, I think we're going to have to have a conversation about those technical standards. Well, technical standards, the challenge for technical standards is it's not enough for AI. And that is because AI also has to understand societal principles and values. A technical solution to AI does not take into account social considerations. That's why we talk about socio-technical standards. It's a new form of standards. Now, what are we talking about in terms of socio-technical standards? Sociotechnical standards are the way to steer future use of AI. So I'm just preempting questions here. What do you mean by socio-technical standards? Sociotechnical standards are a set of grounding to ground AI. And the analogy I often give is around electricity. When electricity was first discovered, it was not used uniformly. It wasn't interoperable. People died, caused a lot of damage. When you read the history of electricity, it's pretty wild about what was happening. Until there was an international grounding standard created for electricity. And all electricity now is built on that underlying grounding standard. And all electricity now is interoperable. We take for granted that we can plug in an outlet or iPhone into any outlet and it works. That's because it's based on an underlying technical standard. So what is grounding AI look like? The challenges we actually need AI to understand the world like we understand the world. They need to understand our laws and our values. An example is often an apple. We could look at a variety of plates of apples on the table and understand that's an apple. But if the AI was only trained on a picture of granny Smith apples, it would have no idea that other apples are apples because it doesn't share our world model or world experience. So how do you actually train AI to be based on the world model? Well this is the sociotechnical standard and where it comes in. It creates a shared understanding, shared common language, shared baseline principles. And these are actually already being developed right now by the IEEE and the spatial web foundation. The IEEE is the international organization that sets standards for electricity, electrical components. It's the baseline standard that's used throughout the world. They are currently right now drafting a spatial web protocol that would develop two components. It's a lot more complicated than this but I'm going to break it down to two components. One is called hyperspacial modeling language or HSML and one is called hyperspacial transaction protocol or HSTP. To ground our conversation in this because when I was first learning about this it went way over my head. But really if you think about the internet today, if you write a web page, if you create a new web page, you have to write it in HTML, the language, in order for it to be interoperable with other web pages. That was an industry standard created when the internet was first invented. It also has to be able to transmit data over HSTP which we all used to have to write HTTP. slash slash www. That's a transmission protocol. It's transmitting information. Every website that's on the internet today has to speak HTML, has to transmit in HTTP. This standard says every AI that is built has to speak HSML and HSTP. So what is HSML? HSML is a method of coding, physical objects, logistical concepts, space. It actually maps the entire world geographically. It's an underlying baseline knowledge that if the AI does not speak HSML, it cannot operate if you're saying AI has to operate on this standard. This gives you an example of what HSML is. It identifies locations, activities, movements, identities. And so Senator Albers, your question of, well, the chatbot said I was something I wasn't. HSML resolves that because there's a baseline explanation of who Senator Albers is built in HSML. And so you ask, well, how do we actually incorporate this into AI that's being developed today? How are we going to convince the large companies of the world to stop what they're doing and all of a sudden adopt this new standard? You don't. You actually don't have to. It states like Georgia. It's the federal governments who say if you want to operate within our jurisdiction, you have to operate in alignment with the IEEE spatial web standard and speak those languages. That actually gives you control over what AI is allowed to operate. This is a USGSTP. This is a universal secure protocol built on blockchain technology that keeps out bad actors in being able to transmit information. This addresses the question of what do we do with the bad actors? What if somebody tries to get in and use it poorly? This protocol, if you are not a verified user on this protocol, you actually wouldn't be able to operate. So an example of this is several weeks ago, the US Senate was holding a hearing on AI, Microsoft's president Brad Smith was there testifying. And he was asked directly, what should we do? If you were to King for a Day, what regulation would you write around AI? He said, well, you should address these larger models first because they present the biggest danger. And Senator Ossof said, well, what about the thousands of AI's that may be coming in and behind the scenes that aren't the large players, but they can do equal amount of damage. And President Smith said, or Brad Smith said, well, you would have to have some type of protocol that governs that if anybody's going to be operating, they have to be on that protocol. What that protocol looks like we don't know. That's this protocol that's being developed by the IEEE. So what it does is it controls what data actually the AI is built on. It also controls how it's communicated. And that's how you steer AI towards the promise and away from the peril. Because the gatekeeper has the authority to dictate what AI is actually allowed to be built. The question of cultural ethics, who's ethics? This actual proposal allows you to build your own local cultural ethics on top of the baseline. So you would have AI that's interoperable, interspeakable. But when the AI from China interacts with the AI from the US, the AI from the US knows the limitations from AI from China because they can actually speak the same language. China has its own ethical issues that they impose on top. And whether you can operate under that protocol to be determined, but it allows interoperability and that's the key. Another proposal that we developed is around an AI ranking system. So this concept of the hyperspacial modal language and the spatial web standards, those are going to be developed. In the interim, how do you actually draft regulation and legislation to take into account the future trajectory of AI? And what we came up with was very similar to what we see in the autonomous vehicle space. So with autonomous vehicles right now, they're ranked on a level from zero to five based on level of autonomy. And this was a ranking system developed by SAE, which is a standard setting organization. And so level one and level two is what you see right now in Teslas, you know, parking assist, level three Mercedes just rolled out, level three in California, Nevada. This actually allows you to get on the highway, press a button, read a book and it will take you home. So, but you're still behind the wheel. Level four and level five are fully driverless. And what we see regulators do is they say, if you are a level three or above technology, you must adhere to these standards. DMVs, for example, say these rules apply to level four vehicles and you have to get this special license level one, level two, you don't. So it's a ranking system based on autonomy that regulators have adopted. That's what we're proposing here as well. An AI ranking system based on the level of intelligence and a level of autonomy that the intelligence is granted. The greater the level of intelligence, the different model of governance that needs to be imposed from the local level. So it's a very sort of technical proposal, but it gives you an idea of instead of just saying we're going to engage in responsible AI today, you're actually developing a framework to be able to take into account future technologies by looking at a ranking system. This just gives you an example of how the ranking system operates. So we have our level one through five ranking system. In the future of AI, you're going to have different levels of AI operating in the ecosystem at different levels. If you actually say as an organization, as a government, level one and level two AI can only operate if aligned with a responsibility framework. But levels three through levels five must align with the spatial web standards because of their level of complexity and danger. Now you can see where there's a network of AI's able to interact through a clear regulatory framework. So ultimately, we believe that standards are the key to unlocking AI, not imposing obligations on governments and people because the technology is going to go beyond what people can do. So how do we actually do, what does this look like in real time? And I'm going to share with you a video to show you how this concept has actually been implemented in Europe with a particular project. So right now, let's go back to the intersection. You have an intersection here in Atlanta. Let's say you have a level four autonomous vehicle operating on the roadway and it enters the intersection and it collides with an ambulance. And we've had this case already happen in California where an autonomous vehicle was blocking an emergency response vehicle. Well, right now, the autonomous vehicle is built by a singular company with a singular piece of software. They're not communicating with the fire department. They're not communicating with the ambulance. They don't know if there's an accident, two blocks away as well that the ambulance was traveling to. Instead, if that autonomous vehicle was built on the spatial web standards and the city of Atlanta granted that autonomous vehicle company the ability to operate in the city based on its adherence to the standards, the city of Atlanta actually could change the rules of the road in real time. And any computer, any car connected to that network would respond in real time to that change. So if the city of Atlanta and this would all be driven through AI, there was an accident, two blocks away, we have to clear these three intersections. And all the autonomous vehicles on the roadway would respond to that direction and not enter those intersections. That's the public safety element. And so we call it code as law. And the future of AI regulation is that laws at the local and state level are going to be need to be written in a way that's machine readable. That's the key to unlocking AI governance. So I wanted to play you this video because this actually demonstrates what this concept of law as code looks like in this concept of how it operates. We'll see if it works. When autonomous machines, like drones, vehicles, and robots operate in the world, their behavior needs to be governed by the laws of the land. However, there's been no way to translate legal text into machine readable code, so that autonomous systems can operate under the most up to date legislation. Until now, hyperspacial modeling language, or HSML, is a spatial web standard, developed of the IEEE. Think of it as a common language that enables AI to model the relationship between people, places, things, and activities. This language is transmitted by the hyperspacial transaction protocol, or HSTP, a universal, secure, and verifiable protocol for communicating HSML between digital and physical systems. With these new standards, legal, physical, and societal policies can be converted into machine readable, machine executable, and machine shareable code. At the European Union's Flying Forward 2020 project, self-governing drones using spatial web standards, and versus cognitive computing platform, successfully flew their mission in compliance with regional laws and policies. Let's explore how HSML enables AI-powered drones to understand and abide by the laws of the land. This process begins with syntactic parsing, and pattern recognition, which identifies time, activities, users, domains, and space, in unstructured legal text. The HSML output, transmitted with HSTP, is what enables the AI system to direct the behavior of a drone in real time. Geofencing, waypoints planning, and dynamic routing, facilitate the creation of flight corridors, no fly zones, and other constraints. Other conditions, physical obstacles, and legal policies such as maximum altitude, speed limits, and validated flying credentials, are accounted for, in real time. Flying forward has been considered a landmark success, setting the stage for all types of AI-powered autonomous vehicles to one-day self-govern. Spatial web standards, like HSML and HSTP, enable compliance to be built into the network, allowing machines to align with our laws and principles, in both the digital and the physical world. To learn more, visit www.versus.ai and download our landmark report on the future of AI governance. So, if we want to go to the last slide here, what do you do next? This is a lot of futuristic, how do you actually address AI regulation? What we're calling for is what we're calling the Prometheus Projects, Prometheus the God of Fire, and this is a public-private sandbox for you as legislators to test laws around AI in real time in a sandbox to see how those would be machine readable and executable by AI that's built on the AI-tripley standards. That's really where we see the next step here. These four states and local governments to be stress-testing different scenarios. You really have to start small. You may start with the intersection example or in health care, but that's ultimately what the Prometheus Project contemplates. So with that, I'll open it up to questions. If you want to read more about this future of global governance concept, we do have a white paper, the video indicated it's on the Dentons website as well. It's called the future of AI-global governance. So we break this down in a lot more detail. I will warn you, it's about 70 pages. There is an eight-page executive summary, which is a lot more digestible, but it's a more of a technical deep dive. So happy to turn it over to questions. Peter, thank you for that very comprehensive update. I'm just going to take that and upload it to AI and have a give me summary PowerPoint presentation anyway. That was fantastic. I'm sure there's going to be a lot of questions here. You mentioned about how our federal counterparts, how they were approaching this, knowing that this is evolving and it would be different than we've done in the past. In some cases, AI may help us write the legislation that governs AI. It absolutely will. It's going to change a lot of the things that we do. One of the things that you mentioned, I'm just going to ask you to expand upon this a moment. I think this is the part where, as we're educating everyone, as to what it can do. I always imagine, you know, it's been in movies or we've all made maybe those of us who went to a big college auditorium classroom. It's a big old chalkboard where the math equation went forever, right? And a bunch of geniuses are sitting around trying to solve the next greatest math. In this case, AI can look at it. Not only can it solve the problem, it can understand math and get three levels past we are. And that's where AI, in some cases, probably already is, but we'll certainly be smarter than us in the very near future. Can you expand upon that type of thought process? Yeah, I mean, I think that's the real risk, is that AI will expand beyond human capabilities. So if we stick with this responsibility framework and we say, well, you should only use AI that you have full transparency and explainability into, you won't be able to explain how the AI solved that math problem. So how can you actually adhere to the responsibility framework? That's why I think standards, whether it's the standards that we talk about or it's another set of technical standards, that is actually the key to allowing that AI to operate but not exceed a certain limit, not engage in behavior that the network wouldn't allow it to engage in. And then I think that's the secret is, governments are actually going to hold the keys to the networks, whether it's a state government, a local government, federal, or really it's going to be a national government that controls the keys to the network and will allow certain AI's on that network based on standards. And that will address the security risk, that will address the risk of that AI, not only solving that math problem, but coming up with a new formula that creates a new type of weapon. And that's what the White House addressed in their executive order yesterday. They've directed all federal agencies to develop standards to address, runaway AI, that will create biological weapon that will create this risk. Do you think these type of technical, socio-technical standards are the guardrail that has to be placed on top of the AI for it to not reach that next level? Absolutely. Questions from the committee members? Senator Goodman. Thank you, Mr. Chairman. I've just got a quick question and I've been sitting here thinking and listening and trying to learn and understand because this is all pretty, I'm not a tech guy anyway as you probably could imagine. I know what you were talking about, Mr. Chairman, about code words, you know, with this regenerative AI and somebody since, you know, a grandmother or a video of her daughter was somebody holding a gun to their head or whatever. You know, I think about those things and then what about a 14 year old girl that somebody uses this regenerative AI and some malicious intent that they create this video of this young girl in some compromising situation or some young boy or whatever else. And then that text message goes out all over the school and they end up committing suicide. That's the kind of stuff I'm talking about in terms of society. If you know what I'm saying, that's happening already. In fact, we passed some legislation called deep fakes to try to address that but the reality is once it's out, you can't unring the bell. You're absolutely right. It is scary. But that type of technology is that people's fingertips right now. Well, Senator Goodman to address that question. The executive order issue by President Biden yesterday calls on agencies to develop protocols to watermark, to require watermarking on generated content, to address that very issue, to have it an immutable watermark on generated content that shows you when you're looking at it, this was AI generated. And that law hasn't been written yet. And this is a lot of the public discussion around the executive order is the devil's in the details. How is that actually going to get implemented? Who's going to roll that out? Who's going to adopt that? But they've directed the Department of Commerce, for example, to develop a watermarking standard for that exact issue. And so I do think that there are immediate things to be done to address some of these high-risk areas, technological solutions, watermarking, responsibility frameworks. But that won't address the future trajectory of the technology because it will go beyond water. AI will learn how to remove the watermark. And so that's ultimately the challenge. So this traditional idea around lawmaking really, our position is that it will hit a ceiling. If it may be not this year, maybe not next year, but it will hit a ceiling because the technology will advance. I personally am a techno-optimist. I think we're all going to figure it out. I think it's going to be used primarily for good. So long as I think there's controls in place. And that's what we tried to address with the papers, answer that question. What are those controls? The only thing that scares me is my granddaddy used to have a saying about people that he would deal with. He said, you got to watch out for that fella because you can't out-figure him. We're dealing with something we can't out-figure with. If you're describing it correctly to me and I'm understanding it correctly. And that's why I think those socio-technical controls, they prevent it from out-figuring us. Because if you, if you as the state control the network, if you say there can't be AI and you can't operate in the state of Georgia unless it's on this protocol on this network. And we control the standards on that network. Then you've prevented the AI that out-smarts you. Because as the state of Georgia, you could set that bar very low. And you could say we're only going to allow AI that's very simple, not very sophisticated. California may take a different approach, but that allows each state to adopt their own approach. And those two AS can communicate with each other, be interoperable, which is the key. The New York Times wrote an article recently about the secret sauce to society or technical standards that are interoperable. Bluetooth, Wi-Fi, outlets. It's all interoperable even though there are different limitations set on each technology. That's really I think going to be the secret of AI operating within the US, for example, in a way that's safe and controllable are these standards. Senator Jackson. Yes, thank you, Mr. Chairman. Thank you for having this conversation. It's very enlightening. A couple of things, if you will, allow me. I just need some basic questions clarified. So oftentimes we hear about algorithms, like Facebook, I've written an algorithm and said that's why I only see certain things. So what's the difference between an algorithm and AI? Are those similar or, yeah, I just need some clarity on them. Yeah. So an algorithm is like a mathematical equation essentially. I understand that, but like, it's this AI operate with algorithms and sometimes it's different. Yeah, and AI is, you can imagine, it's layers of algorithms on top of each other at different levels. So the more complicated the AI gets, the more algorithms that are operating on different levels of information. Social media algorithms, for example, are making recommendations based on data that you've inputted into the system. The AI's that are out on the market are doing the same thing. Some of them make recommendations, but they can also generate content based on prediction and assumptions or assumptions. So for example, that Microsoft Co-Pilot tool, it's looking at all the data in your organization and making an assumption as to how you would like an email drafted. It's an algorithm that's processing that information. So they're kind of used synonymously, but algorithms are kind of like the line code that's being used. One of my favorite SNL skits is when they were doing a skid on Facebook, it was a Facebook scandal and they were doing one of the skits with the opening scene and the guy playing Senator Kennedy. He said, now did you bring the algorithm with you today? Now how big is that? That's a big old algorithm. So it is used interchangeably. Okay, that's helpful. And then, so you gave the example about the interfacing of the autonomous vehicle with the ambulance and the stop light. So we have Marta buses that, my understanding is they can already kind of interface with our traffic signals so that mass rapid transit can happen a little more quickly. Is that, I'm trying to figure out what's already AI in my life? So is that, is that AI or is that automated in some other way? Some of it. So a lot of it is that technology that you're talking about, a lot of it is actually, like when you're going through a security gate, you've got your code. These buses, these ambulances, fire departments have technology built into the dashboard that can send a signal to the traffic light to make them all green. What the challenge is is that if you have that technology running and then you have a driverless car entering that intersection and you have autonomous food delivery vehicle, this tiny little robot, and if you've seen them on the videos, that's entering the intersection. How do they all communicate? And right now they're on different networks. So the bus is communicating to the light through sometimes it's a tag sitting on the dashboard, sometimes it's a computer program, depending on the sophistication of the traffic light. But if that's sitting on its own network, governed by the city, and the autonomous vehicle rolls in, and it's on the car manufacturer's network, and the food delivery is on the food delivery network, they're not speaking to each other. So this gets back to the concept of, there needs to be a single network for that intersection and permissions granted to enter and exit. And in the future, the idea is that that signal that the bus is sending to the traffic signal, that signal will be received by other participants as well, and they would know, stop, make a change, and that's where the law's code comes into play, because when that signal changes, there would actually be code sent to all those vehicles to change their direction in real time, without human input. That's the idea.\n\nof it to be fully autonomous. And then my follow up to that is, you know, I hear this language about law as code. I mean, we are law makers. We make laws. And then there's an entire another body of that interprets that law. And so if we make a law that's code, who's the interpreter? Is the AI the interpreter of that? And how do we ensure that they're interpreting the law that we, with the intentions and the thoughts that we put into it? How do we ensure that they're interpreting it in a way that is appropriate and good for society? That's the critical challenge is that this concept of drafting laws and regulations that are machine readable and machine executable potentially may require a different way of writing law in a way that explains intent and behavior. It may have to actually get more granular and more specific, not principles like act reasonable in an intersection, but actually get very specific as to behaviors that are permitted and behaviors that are not. Now that runs obviously and I've had this debate, every time I talk about our proposal, the debate is about what do you do about common law, jurisdictions, and civil law countries. And we have case law and we have soft law and we have interpretations by judges. That's a challenge in developing that type of protocol and encoding law into machines. And right now autonomous vehicles face this challenge. For example, in California, the California DMV tells vehicle manufacturers you need to comply with the vehicle code. We also have the drivers handbook by the DMV that's kind of soft guidance that I'm sure everyone follows at all times. And we also have case law that talks about what is negligence when you engage in a certain road type situation. How do you encapsulate all of that knowledge and build law into those systems? And I think that is the next step. That's the next challenge. And AI may be used to actually solve that problem. Then one final, just one final comment, listening to this conversation and hearing all the things about the dangers of it, it reminds me of what I imagine people as they were thinking about the creation of a vehicle of cars, the fears and the anxieties that came with that. Because it is dangerous technology. And one of the things that we do as lawmakers, yeah, there's some guidelines and principles that we've put around how we will operate vehicles together as a community. And when those things don't work, we come back together and we make more laws to help. Because we learned, oh, we learned, oh, people like to drink while they're driving. So we should make a law around that. Or people smoke marijuana so we make a law. And so to me, where we're entering into a places, where we're just going to have to keep coming back as we learn more about the technology. And yes, it's scary. And operating a heavy, heavy vehicle can be scary. But think about how it's changed our world in so many wonderful ways. So I'm actually really excited about this. I'm really looking forward to autonomous cars. I think we're all going to be safer as a result of them. So I can't wait to see them in Georgia. Well, Senator Jackson, I'll give you an example of where how do you actually come back and write the law in this way? This is where, for example, the Prometheus Project and simulations come into play. So imagine as legislators, you can come up with a proposal and say, we're going to draft a law that says, you cannot do X with your AI system. What if you could simulate that a million times in a simulator with real world digital twins of the city of the intersection? And you find out all of those two million simulations that we just ran, 80% of the time there's an accident. Because somebody always turns left unprotected based on the data that our city has on traffic patterns. Now you have the data to actually adjust the law to actually reflect real world scenarios. And that's where AI will help you develop those laws and come back. It's going to be data in, data going out. But ultimately, who controls that network upon which AI sits? I think that's the ultimate answer rather than the AI itself. It's the network upon which it sits. Other questions for Peter? Senator Jones. Yes. Thank you, Mr. Chair. So I have a couple of questions. I'll put them together at the same time. Number one, are the regulatory agencies kind of discussing the fact that they're going to have to change the way they do their rulemaking? Because it's probably too long the way they currently are doing their rulemaking. It takes a year or two. It's already going to be passed. And then number two, which you're kind of touching on by the courts, does it seem like we're going to probably at some point in time just have to create an AI court sort of speak to kind of deal with these kind of issues just from a knowledge base and also just understanding it. Yeah, I don't know about the AI court, but I like the idea. In terms of the regulatory agencies, they are having those discussions. Senator Schumer, in particular, just at the Senate level, has said we cannot draft legislation in the normal course to address this technology. And the FTC is currently looking at rulemaking, the EEOC as well. I was just at a conference where the head of the autonomous vehicle group for the California DMV was presenting. And I asked him that question. I said, are you all revising the vehicle code or drafting new versions of it to address driverless cars? And he said no comment. So I do think those conversations are happening, but there is a dearth of experience in terms of how do you actually go about and redrafting those. In terms of court decisions, I actually think AI is going to play a large role in the legal community because ultimately anything that's knowledge based, AI is going to supplement or potentially replace. So the days of doing legal research on West law or Lexus to find the right case, I actually think that's going to be replaced by AI because it's knowledge based. There will be a tool that's verifiable and finds accurate case law. It's the judgment part. It's the interpretation part that AI currently can't do that. But they're in the midst of building AI that can have rational thought and make judgment calls. And that's going to be the next stage. So I'm not talking about like AI court to replace a judge what I'm saying is as far as the body of law is concerned, because our judges in over 50, they've never even taken a class dealing with AI. So as far as specialization, we talk about business courts being specialization courts, should there be a specialized courts to deal with some of these legal issues as far as judges are concerned and even their attorneys that can practice it too? Potentially. I don't know that we'll see new courts necessarily created just because of the constitutional challenges and at least on the federal side of Article 3. At the state level, what you might see similar to what we see in state courts right now is certain departments being designated as the expert department. So for complex class actions in California, for example, each city court has their complex class action department with a judge that's all they do. So I can certainly see that direction with this type of technology, or perhaps something like a Pfizer court where it's a particular expertise with a particular body of law. Certainly an interesting proposal. Great insight, Senator Jones. Senator Dugan. Thank you. IEEE, you said have the state enforce a standard that you use the IEEE standard on this? I think I'm paraphrasing you right there. How does state enforce that? I mean, we can say anything that the state uses has to be that standard, but not everybody in the state would have to. Yeah. So what you would rely on is, for example, if the state wrote a law and said, all AI developed into pulling this state has to adhere to the IEEE spatial web standard. Let's take, for example, what happens when a state says this company's cybersecurity program must add a baseline adhere to the NIST cybersecurity framework? Well, states require when they investigate, so whether it's the California Attorney General's Office investigating or the new California Privacy Protection Agency, they'll ask the company, show us your documentation, your certification, your third party verification, your governance program, showing adherence to the protocols. The protocols themselves are actually protocols that you can map against. So how are you mapping against as a company? Show us proof of compliance. So it's going to be very similar to other areas. Now, whether you win a company, for example, seeks certification to enter the city of Atlanta's network, what documentation or proof are you going to require that they're adhering to those standards? That, I think, is an open question. Whether you require documentation, verification, and signature by the CEO. But that's ultimately where I think it would play out. Peter, wouldn't that be similar though to some of the data privacy standards that are put out there, obviously, California, Virginia, Colorado, places like that? I mean, we put similar standards out there at some states have adopted it. Sometimes there's a federal guideline. This is going to be similar to that only on just such a massive scale. Correct. An example of that is Tennessee. Tennessee passed an omnibus comprehensive data privacy law. The first state to do this, by the way, where they wrote into the law, if you're investigated for violating this privacy law, if you can show that you were mapping yourself to the NIST privacy framework, then you have a safe harbor against enforcement. And so that was seen as, first of its kind from a state doing that, but it's using a standard to benchmark a particular activity. And it allows the state to actually draft the law and then put the burden on the private sector to prove compliance. So it doesn't actually radically change how laws are written and enforced today, but it's just what's the standard that you're measuring against. Now, this concept of creating a network upon which the city governs, that's obviously a much larger technical discussion. But as at a baseline, if you said, if you want to operate in this state adhered to this standard, that's a pretty tried and true regulatory approach in other sectors. OK. So one of the other things we're thinking about this advances in automobile AI is going to do away with high-speed chases. You shut the thing down. Absolutely. Until they hack it. Until they hack it. And if they're that good, then hopefully they can find another job. I got a thousand questions, but I'm only going just a statement and a question. Our questions really. What happened to Prometheus? That's kind of my point that I've keep asking all along. We've got to be careful with the consequences. And then ultimately, and I don't expect you to answer this one, I saw it all the way through. Ultimately, what's the purpose of humans? If your last slide was eventually in the near future, and when I say near future within the next century, AI is going to be able to do everything that a human does now, then what's our purpose? I think the answer to that last question is that's why standards are necessary, because you can put a cap on how far the AI is going to advance. And as a society, if we determine, we don't want AI to replace the sector. We do not want AI to displace cab drivers and Uber drivers. So we're actually going to put a cap on how autonomous the technology can be in our jurisdiction. I think that's the answer. And so it allows local jurisdictions to make that decision as to what the role of human beings are going to be. So West Virginia made a decide, we don't want any AI, because our industry is heavily human dependent on mining or physical labor. And California may say, we're going to be the AI capital. That's OK, if everything is on a baseline standard. And I think the concept of Prometheus is that's why controls are necessary. Because if you don't have those controls, you will have runaway AI. It just will happen. And so that's the concept of a standard-based approach, regulating the network upon which AI sits, rather than relying on people to develop AI responsibly, I think that's ultimately going to be the key. How we get there, a very complicated conversation, and happy to answer all 1,000 of the questions afterwards. Any other questions for Peter? Thank you for your excellent presentation. We appreciate it. Thank you very much. Appreciate it. Appreciate it. And Maria, if you'll come on down and recognize, we're fortunate to have our Senate Majority Leader here, Steve Gooch in the back of the room. Thank you for joining us, Mr. Leader. I am in the artificial ability. I either should. All right. Thank you very much. Good afternoon. We're no longer in the morning, it seems. I really appreciate the leadership and the members of this committee for allowing me to speak on this exciting topic. And going third and last means you'll hear a lot of the same thing. Some iterations off some of the same definitions or stats. And then I hope a little bit of a lighter presentation after my two friends here. But my name's Maria Sob. I am a senior public policy manager here on behalf of Amazon Web Services. I get the pleasure of covering Virginia, Florida, and Georgia in my work. I am here to talk to you today about AWS's perspectives on artificial intelligence and machine learning, specifically how we understand the various concepts and technologies developing in this space, how we use these technologies as a company and what we offer to our customers, and how we see regulation evolving in the coming years ahead. I think it's important. I know some of you are members of the technology committee, but I always find it's important to talk about what AWS is. For those of you who are unfamiliar, AWS is Amazon's cloud computing business. Approximately 18 years ago, we realized that we had developed a unique IT infrastructure to power Amazon.com that could be shared externally to empower customers in the technology space. Cloud computing on its own is the provision of IT resources over the internet. And this forms a very critical foundation for the use of AI and machine learning. In growing our global infrastructure of data centers based on our initial realization 18 years ago, we've evolved to providing over 200 unique services in the cloud to millions of active customers worldwide. These include a diverse group of individuals, organizations, enterprises, startups, and governments. And in the spirit of today, this has enabled us to extend new capabilities in the AI, ML space. The focus of AWS's development of AI machine learning technologies is in line with how Amazon does everything and in how we provide our cloud services. We are customer obsessed. We are offering the building blocks to customers so that they can develop the programs, applications, or products to their end users and to the world at large. So here's my agenda for today. I'm hoping to cover a few topics. It's going to seem like I'm taking us to the very beginning or the basics, but hopefully that's still helpful. Please do, Maria. We have to hear things three times. Same. Same. So I'll try to go over what is AI and what is generative AI. Amazon Web Services experience with AI machine learning. What is our version of responsible AI and how we see participation in the regulatory efforts in the years ahead? I have to say, Amazon's a writing culture, so using PowerPoint is almost like a, harking me back to a different time. OK, what is AI and what is generative AI? Well, I first want to talk about what it looks like for us at Amazon, and I'll talk about this a few times. But for the last 25 years, Amazon has invested heavily in the development of machine learning and fusing these capabilities into almost every business unit everywhere in the world. The technology is used both for customer facing services and internal operations. From the recommendation engines that personalize your shopping experience on Amazon.com to the AI-powered robots that work alongside, for example, the 30,000 Georgians we have operating in our fulfillment centers every day. We launch entirely new businesses focused on AI, like Alexa and some of the smart devices you use often, the Echo device. And we play a critical role in the widespread adoption of AI in other industries. Like I mentioned before, AWS is a key component of that. And even today, we are offering over 100,000 businesses the use of machine learning services. Despite the seeming novelty of recent AI applications, these two technologies are not new as you've heard from my friends before me. Every day technology, like auto correct, email spam filters, background noise dampening, and video calls leverage AI and machine learning. And we're continuing to do that and excited about what lies ahead for us as a company and how we support our customers. So these are easy statements around AI machine learning, deep learning, and generative AI. At AWS, we talk a lot about the AI machine learning stack. So you can see how incorporating different components of the conceptual technologies may or do end up leading you to concepts like generative AI. Recent advancements in the machine learning space have contributed heavily to generative AI. And AWS is making large language and foundational models more accessible and efficient for builders and customers through a number of our services like Amazon Bedrock and Code Whisperer. Our approach is to hopefully democratize access to generative AI, but also provide a choice for customers in every industry to me any use case. So again, I brought this up a little bit, but the way that we are looking at using AI machine learning is to almost provide, is not almost, is to provide the developer with the tools to build out their vision for how AI machine learning is utilized. We are not providing the end product. So when we think about how we support customers, we see a lot of potential for innovation, for use specific cases, and we are excited about what might be ahead. So this is a little bit deeper on what is generative AI, because I think this is both the scariest and I call it the sexiest part of artificial intelligence and machine learning. But generative AI creates new content and ideas, including conversations, stories, images, videos, and music. It's powered by large models that are pre-trained on a lot of data and commonly referred to as foundational models. So with a lot of potential, why is this technology kind of seeing so much interest now? And we believe it's kind of reached its tipping point. The convergence of technical progress and the value of what it can accomplish. We know that there is massive proliferation of data and the availability of highly scalable compute capacity with advancements of machine learning technologies. This is why we're seeing generative AI takes shape. While we're seeing this great momentum and evolution, I also just want to call out, because I know we've talked a lot about the scary components of AI. It is here. You're using it every day. But when we think about how we're harnessing data, I always harken back to a stat that our former AWS and current Amazon CEO Andy Jassy Sites, which is only, it's about less than 10% of global data lives in the cloud. And that doesn't mean just on the AWS cloud, but in clouds generally. So if you think about how much data is necessary to utilize AI to its fullest capability, we're still very much at the beginning. If we think about public sector agencies and state governments, federal governments, universities and schools, some of them are in the very beginnings of their technology journeys. I get the pleasure of working with states that are still using systems from the 70s. So you can imagine that getting to AI still will require time planning, budget, and wonderful people to support in the development of that workforce. I want to talk about some use cases here that we see in supporting our customers. And I have some fun examples. But what we're excited about is the potential, certainly. I have a slightly different stat than my friend at Dentons, but we are anticipating at least a $7 trillion increasing global GDP over the next year due to the use of generative AI. And then, which is exciting, oops, sorry, should go back. But it gives really interesting economic opportunity for individuals and organizations alike. And I think one of the most interesting questions is what would be the risk of not using AI machine learning in everyday business operations. Some examples I want to talk to you about are ones that are more everyday. So if we look at the category improved business operations, one of my favorite examples of the use of AI machine learning is in the agricultural space. I know agriculture is very critical for Georgia and many Georgians who work in that field. I don't mean that pun intended, but you know. One of our longtime customers is John Deere. An example of how they're harnessing cloud computing and AI machine learning skills is through Amazon lookout for equipment, which is an AWS service. This allows them to connect over 500,000 pieces of machinery and equipment into the cloud. And then John Deere can utilize machine learning to detect abnormal equipment behavior or to better anticipate equipment maintenance, which ultimately then saves them significant amounts of money over time and ensures safety of their employees and productivity on whatever land or a project that they have in place. We are working with the state of Georgia on leveraging some of those technologies in the AI machine learning space as well, which are in pilot or beta form, but will allow for greater efficiencies as well. And we want to just congratulate the GTA and Georgia as a state. For AWS, we get to champion Georgia very much on main stage showcases. They are a top cloud adopter in our eyes in the amongst the remaining 49. So I just do want to commend them on a cloud adoption and the move there, which will enable them to move faster into the cloud. I also want to emphasize the need for workforce development. We've had the opportunity in Georgia to support workforce development and education initiatives focused on cloud computing, which I mentioned before is foundational to the use of AI and machine learning. We launched a commitment with the technical college system here and the Department of Ed in 2021 to skill 5,500 learners in the state. For free on cloud computing, we successfully made that target one year ahead of schedule and we've iterated that program to offer our training to existing Georgia employees, which will be critical for the move to public sector adoption of artificial intelligence and machine learning. Like I mentioned before, at Amazon, we're using AI all the time. We use AI machine learning to optimize our logistics routes and the robots that operate in our facilities. We're about to hit, I mean, I guess November 1 is officially the start of the holiday season. So we'll be ramping up on our compute power and moving those robots quite a bit and ahead of Black Friday and holiday shopping. We use our own services to develop our innovation. So Amazon Alexa utilizes an AWS service around natural language and we use machine learning to continue to optimize how she converses with you, hearing what you ask her, what phrases and that extends to unique colloquialisms. We have Alexa enabled in dozens of languages around the world and it's been a really wonderful tool, especially someone mentioned access to seniors. Some of these technologies that are complicated but encompassed in a simple format like an echo dot can be a critical component of someone's day. So whether that's to get information about local elections, whether it's the news, listening to music, enabling a face-to-face chat with family members, calling emergency services, asking a question, we see the real benefit to our customers and the basic technologies on AI machine learning. All right, so what is responsible AI? And this is going to plague all of us for a long time, I think because there will be many people like myself, companies like Amazon, think tanks, you know, national bodies that will come with their version of responsible AI and all of them are valid perspectives that should be debated and discussed openly and transparency, transparently, to move us along in the use of this technology. Oh, sorry, for us at AWS, we define responsible AI in six key dimensions. They are bias and fairness, explainability, robustness, privacy and security, governance and transparency. While this is our definition today, we know that we will continue or need to continue to iterate on what this means based on how AI machine learning are being utilized, what the science and engineering looks like and the conversations that are evolving and taking place. And so like my friends before me, I developed this presentation before the White House announcement, so I'm sure that will spur in different presentation if you asked me a week from now. But to hone in on the categories I mentioned, I'm going to just give you some definitions to consider or perspectives we have. On bias and fairness, this is how we consider how a system impacts different subpopulations of users, whether that's by sex, ethnicity, language, religion, et cetera. We want to make sure that there are not harmful disparities in the system performance across these subpopulations. Explainability, that is whether the system offers a clear rational decision for, or rationale, I'm sorry, for its decisions. Customers want justification for the inference produced by an AI system, which is particularly important for those when we discuss compliance requirements. Robustness. We'd like confidence that the AI systems cannot be easily fooled or confused. What success means here is that at least we have understandable guidelines for when the system is expected to work. Privacy and security is paramount since we are using tons of data, often personal data in some cases. But this is for us one of the highest priorities. Use of machine learning brings potential risk to customer content and Amazon IP that may require preventative and mitigating steps beyond those taken in the normal course of protecting privacy and security. And then finally, I'm sorry, almost finally, governance, processes to enforce and ensure responsible AI practices are being carried out by stakeholders. And then finally, transparency. And this is the extent to which an organization enables customers and end users to know whether or not they're using machine learning based applications and to make informed choices about their own use or interaction with the service. So one way we are doing this, because I mentioned to you before, AWS is providing the building blocks for others to develop their own applications and programs. And so we are most commonly working with developers on the use of the technology. And one way that we've done that or encourage responsible AI when interacting with these individuals is through the use of these AWS AI service cards. These are a resource to our customers and are a form of responsible AI documentation that provides customers with a single place to find information on the intended use case and limitations of the service, responsible AI design choices, and deployment and performance optimization best practices for our AI services. One of the reasons why we have developed the AI service cards is culturally similar to how we offer cloud services, which is our customers utilize our cloud services based on an acceptable use policy. Often there is a contractual agreement in place. And implicit in those is the expectation that our customers will be using our services in accordance with the relevant or jurisdictional laws in place. So this is an added layer beyond our cloud services premise, which is this is how we expect you to use these services. And this is what\n\nyou can do and the parameters we believe exist today. We will be continuing to optimize and iterate on the service cards. We're at a really exciting time. At the end of November is our annual AWS reinvent conference in Las Vegas. I didn't invite you all, but I probably would be not in compliance. I'm from an ethics perspective, but please go if you care. But the hope and expectation is we'll announce a myriad of new AI services and AI will be a hot topic. So I can imagine how these transparency cards continue to take shape. All right, I just have two more slides. So, and this is a good stuff that I know most of you are most concerned with, which is how should we, or how do we think we can all be playing in this regulatory space? So this is a global conversation and a national or state conversation as well. And we are, as AWS, are participating everywhere that we feel like we have a good place to participate because technical expertise and context do matter. We're seeing an increase in the number of regulatory proposals, laws adopted, standards developed, and multilateral agreements negotiated across the world. There are hundreds of proposals expected globally just in 2023, and we're all about to head into session. So I'm sure everyone will get their chance at an AI piece of legislation, which hopefully will keep me employed for a little while longer. But one thing we would say is while there is an interest in wrangling the technology through new legislative or regulatory regimes, one perspective is that many of our laws on technology today and industry specific or structural specific regulations may already cover the use of AI machine learning in the course of day-to-day life. So some things to consider is when you're considering whether a new regulation is needed, you should ask yourself whether the use of AI adds new risk beyond the risks already present when the AI is not in use. And then where existing laws are insufficient to address application of AI, why don't you consider whether a mending existing apical requirements may be better than creating a brand new or separate framework. Working within existing regulatory frameworks and with existing authorities may actually help to ensure that sector and product specific AI expertise is being developed and will facilitate regulation that is targeted to actual issues at hand in that kind of use. So it goes to the point that was made earlier about, risk-based analysis, there is a theoretical conception around AI regulation, which is whether it should be horizontal or vertical. Horizontal meaning, we just have an AI regulation or standard that covers the use of AI and machine learning across the board, regardless of what industry or specific use case. Vertical regulation may mean let's hone in on AI machine learning use in the healthcare sector or in the financial services or in the education sector and then developing specific regulations based on that. Finally, I just want to reflect on some things that Amazon and AWS are considering as we think about these policies. There are, you know, again, we're at the beginning. New questions will emerge, but content moderation and Mr. Information, copyright, cybersecurity, privacy and security, labor issues and discrimination are top of mind for us as just basic categories. Some additional examples when we go into them, you know, content, moderation and misinformation, we've talked about deep fakes, manipulated media. We are concerned with how maybe services will be used for that. Copyright owners, you know, Amazon has a number of Amazon creators at leverage amazon.com and AWS, which we love, but we do have concerns about training models using copyrighted materials. We care about, you know, discriminatory outcomes, leveraging services. I think this is an area that has been studied a little bit longer than some of the use cases we're hearing about now, but it's certainly not a complete study. And so we are continuing to, like, sorry, continuing to learn about ways that data can be optimized to eliminate bias, hiring engineers that are diverse to ask different questions and then ensuring that, you know, our products and services are available to all kinds of individuals so that we get new conceptions on innovation. And then I'll just end by saying we're participating very publicly in conversations about responsible use of AI. We participated in the development of the NIST framework that was mentioned earlier and we are taking an active role in forums that will, you know, be interested in AWS's perspective of colleagues who are in the UK today and had colleagues at the White House yesterday. So, you know, we are excited about being part of the AI machine learning evolution. And we hope that, you know, we can continue to be a good partner to Georgia and figuring out what might be ahead. Thank you. Thank you, Marie. Another just outstanding presentation. What I really like what you brought today is most folks in this room probably think of Amazon as, you know, the folks that deliver 12 times a week to the Alps house. Yes. And don't even understand what AWS is and what you provided for companies is the ability to really have unlimited growth and allow them to get out of their non-core business, allow you all to handle that and let them the grow their other business. But you're such a critical part of the overall ecosystem as both Peter and Fred had mentioned earlier. So, I think that that is critical. I'm going to ask you a little different question than we've been asking other people because a lot of this is going to be happening on AWS's servers, right? And it already is already, but I see that as we were talking about earlier, this limitless growth, right? It's going to just continue to grow beyond what we can even comprehend. Do you feel that, you know, Amazon is going to be able to be able to prepare to how is what's going to be an on the limit amount of data centers and storage that will need power and electricity and all those things of that nature, because we're already seeing that as being a challenge in today's environment and then knowing this is this growth is so significant. Yeah. So, just to hear your question back is, what do we have infrastructure in place to really harness the use of AI machine learning in the ways that we're all kind of contemplating? And the question is yes and no, right? So, yes, we aspire to have a global infrastructure that is modern technology enabled, and that goes beyond just data center infrastructure, right? We think about the proliferation of electrical vehicles. You need charging stations, those need data. There's the use of smartphones. We need electricity all around. We recognize our thoroughways and our transportation systems also need modernization. So, the use of broadband has been discussed quite a bit since COVID-19, and I think there's a lot of infrastructure modernization that does need to take place to fully harness it. I think how quickly AI machine learning will be adopted is still, you know, a lot of us talk about the pace is unbelievable. But I do really believe that we're still early days for really rapid adoption. If we just think about the use, again, more so in the public sector, but certainly even in some private sector organizations, there are several steps that need to take hold. And I think when we fully understand the demand signals will be another trigger to how infrastructure develops. But I think everyone wants a modernized world that's fast, efficient in our hands, in our homes. And so, I think we'll be motivated to deliver on those services. And when we switch gears and ask you one final question under cybersecurity, obviously, you know, one of our greatest threats today affects something that the Georgia Senate did a lot of work on in this last session. One of the benefits, I believe, is the cyber security world that's embracing AI. The AI hopefully stays ahead of the bad actors. In some cases, they're AI. I almost see a battle of AI is from the good and the bad guys over time. I'm curious what your thoughts might be to that. Well, you know, I can only imagine who the bad guys are. But I do think that cyber security does play a critical role in mitigating risks for whoever those bad guys are for the day. You know, we use machine learning to understand how our customers are leveraging our data in the cloud. So if there are anomalies in data usage or capacity usage or spikes in cost, right? Because cost is often determined by usage, at least for us at AWS. We use machine learning to identify those risks. We also use machine learning to help identify risks of when an S3 bucket is left open. And then that will trigger notification to the customer. So we see cyber security being very much strengthened by the use of artificial intelligence and machine learning. I think there will be, you know, when we talk about good verse bad. And I think I hate to even really dive deep on good verse bad because I think it will continue to change. But one thing that I reflect on is the interoperability of systems. And I think that how truly interoperable systems are will need to be discussed. If we just think about, you know, filling out forms at a doctor's office, like let's not even be digital just thinking on paper. What you provide to one doctor or how you fill out their questionnaire is so different from how another doctor might ask similar related questions. And then when you're trying to put those together to aggregate a view on a patient, well, they might not even be compatible because they haven't even asked the same question. It's very difficult, even for us when we talk about cloud migration of multiple systems, maybe even across some of the same systems in one government or even in a company to get harmonization of data or perfect data movement. So I think we're early days on kind of how good AI versus bad AI compete in the virtual space. But I'm sure it'll come up. Fantastic. Other questions from the community members, Senator Islam, that number four. OK. Hi. Thank you, Chairman. I just want to say I've learned a ton today. So thank you so much for all these presentations. As you mentioned, we want a modern society. And we want a modern and safe society. So I believe with the new words and slogans I learned today, as we keep humans in the loop, you know, we can establish sociotechnical standards at the state level in order to ensure that we can have a safe environment with AI is what I've been understanding. It's sort of like an industry. And so Georgia could be the superstar of AI if we wanted to be. And it's very much individualized is what I can tell. Or it can't be. I guess, well, that was my comment because I've learned a lot. And I want to continue to learn. And the other thing is, and we talk about how AI can modernize our lives and make things easier. I would love to learn more, maybe not right now, but later on, on tools that local government state governments can use to help optimize, whether it's research, constituent services, or just making government more efficient. Yeah. I think they're really, they're far, I'm going to use simpler or easier ways for government to experiment with AI machine learning, which can be funded in the form of pilots. We've seen a number of states do that. I get to work in Florida. We are piloting a program around the use of a data lake to share information across key stakeholder groups that deal with students who are need mental health or medical attention. And so one of the issues they flagged is the state can lose millions of dollars and significance amounts of time in treating a student that is facing a mental health episode. And that ultimately leads to poorer outcomes for that student. And harnessing a data lake with embedded AI machine learning capabilities can allow different entities to communicate in real time, get a clearer picture on that individual's profile, and lead to those cost savings over time. So I mean, that's still kind of a complicated scenario, but it's a simpler use case than some of the examples we've all discussed today around AI machine learning. I think, again, I mentioned workforce development and education opportunities. Georgia has wonderful existing institutions that are really focusing on technology skills. Not just for young students. We talk a lot about pipeline. And that usually in our minds starts at kindergarten. But skilling and re-skilling individuals is critically important. I think providing public sector workers, Georgia State employees with training programs is critically important. There's a lot of big focus on, we talked about institutional knowledge and losing some of those people who've worked in technology or IT for decades. And there's no reason why they should be left out of the technology story. If they're interested in learning and taking on new skills, those should be available to them, even if their expertise is not generative AI or chat GPT. Other questions from Maria? Senator Dugan. I'm sorry. When I'm taking in and I'm learning as quickly as I can, it does generate questions coming along here. So I guess I'm becoming Senator yet. But humans are human. So I saw one of your slides that industry is actually helping draft some of the regulations are constraints going forward. I've never seen a corporation that was not at least partially self-serving. How are we making sure that when they're helping chart the future, I think was the term you use, that there being selfless in that approach? Yeah. So I take your comment or question. I would say that from our vantage point, we're building services that we're working with customers on. Amazon's a very customer-obsessed company. Everything we do is focused on even the individual customer. And the way that our company ethos is is that we work backwards from even just one customer who even might use Amazon once a day or once a week or once a year. We're really honing in on the individual. So I would say that our perspectives on technology development are focused beyond just Amazon. We're focused on the millions of people we serve worldwide. We get key insights from our products and services that allow us to have a more intimate view on kind of how the technology should develop, can develop, will develop, and that really builds out our interactions. We certainly, everyone who is coming to this space believes they're an AI machine learning expert. They know the latest and greatest. And that is certainly true. But as you've seen with the three of us, we all have slightly different perspectives. So I think we'll all balance each other out kind of in the ether over time. OK, I'm just two more. We talk labor up there. And I've heard it a couple of times. It'll help be less discriminatory in hiring practices. I'm paraphrasing a little bit, but that's not hard. About firing. So I would say it goes both ways. We are interested in how policies will be used both ways. Certainly, we're seeing a lot more automation in how employees clock in at work. Or with remote work, you have probably seen articles about companies doing remote monitoring of desktops or laptops. So we're hoping that the information that is provided into some of these systems is being used fairly with explainability, with transparency, to allow proper recourse for individuals or organizations. So when we think about hiring and firing or labor issues, it's really holistic. It goes always just to make sure that the technology is being utilized in the most predictable and most transparent way as possible. And last one, and for Gidman, I come from a military background. So I look at everything as we're going ahead, but where are vulnerabilities? Sure. Countries that have a very authoritarian AI program versus countries that have a very open AI. Let's say hypothetically, there was a belligerent country out there that had a very closed AI. But one of their potential opponents was very open. It seems to me like that's a massive vulnerability. I'm trying to figure out the question. The question. Like what is a recourse there? Yeah, it really comes down to how we advance, but protect ourselves as an institute at the same time. Sure. So one thing that I reflect on is that we're an American company. So first and foremost, we're an American company, but we have a global presence. And we serve customers globally worldwide. So we work to be compliant with all the laws and regulations that exist everywhere that we are. But first and foremost, we're an American company. We have great relationships with the US government and ensure that considerations around things like national security are first and foremost when we learn about incidents. I think anything over the internet is subject to foul play. There's always a risk of a cyber attack, a hacking, misuse. And a lot of the data sets that we are using to fuel AI machine learning are open. And that's been actually the hallmark of how we've gotten to these innovations. We get to test synthetic data. We get to test data sets in exciting ways. Some of our leading research institutions provide those. But I think ultimately when it comes to closed AI versus open AI and the data sets that are involved there, it will really be about making sure that we understand how data is provided, what data is being provided, the security of those data sets, and eliminating vulnerability at those levels to ensure less vulnerability at the larger scale level. Yeah, I have no doubt in my mind there's the shower doing it the right way. I'm more worried about an international state level. Yeah. Well, I mean, I think throughout this presentation, I've thought about good use, first bad use. And everything is a good use and bad use. If you have a baseball bat, you can use it to have a really memorable day with teammates and friends and fans and watching a baseball game. But you also know a baseball bat can be used as a violent weapon and destroy someone's life, right? So these are tools that will have good and unfortunately inevitable bad uses because people will have access to different tools and the mind is an inventive place. So I think we are here to ensure that more good uses are ahead than bad uses. But we know that that will evolve over time. Senator, I'm going to piggyback on what you said a little bit. I think that's happening in some cases today, right? If you look at certain countries, there's a big China in the Middle East who have locked down a Russia. State-run media, they've locked out people for certain access to the internet to what the rest of the world sees and kind of preach their own, whatever they want to call it. But they box out America. We tend to see it all the other way just because we're more open in our communication. So I think what you're saying, though, is that's taken that put on steroids now, right? OK. And just one of the things that I'm going to go ahead and rewrite a version of Romeo and Juliet where 20 years from now, a super intelligent AI falls in love with the human. I think that movie was made, right? I don't know. With Joaquin Phoenix. Yeah. Yeah. You're too late. It's not going. If I only had AI, it's going on later. You could self-publish that story on Amazon. So you know. Yeah. Help me write it. Other questions from Maria. Maria, thank you so much. Thank you so much. You really appreciate it. OK. Folks, I know we went over in our time, but I'm so glad that we did. I will tell you that is my belief in our lifetime. This is going to be the single biggest demarcation of time and innovation that we're going to see. And it is going to change almost everything in our lives. Now moving forward, especially for our children, our grandchildren, and beyond. Again, our next meeting is going to be during special session. As soon as we figured out the exact timing of different other meetings, we're going to schedule that for those that have not met Alice and Bailey. In my office, she's been here for about two and a half months now and doing a great job. If you are interested in being a speaker or participant in this committee as well, please see Alice in. We'll be happy to talk to you about that. And please pardon our dust here in the Georgia State Capitol. We've done a massive renovation over the last many months. It's going to look fantastic as it all gets done here. But as you leave here, please watch your step. Some of these stairs have a lot of dust and other debris on them. Elevators are available. Thank you for your time today and moving forward. And we will call this meeting here by adjourned. Thank you.",
  "updated_at": "2026-02-13T17:04:06.357385+00:00"
}